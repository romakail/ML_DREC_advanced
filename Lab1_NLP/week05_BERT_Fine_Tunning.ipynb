{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNKaJz5j_ylj"
   },
   "source": [
    "## week05: BERT fine tunning\n",
    "*Based on [BERT Fine-Tuning Sentence Classification notebook on Colab](https://colab.research.google.com/drive/1ywsvwO6thOVOrfagjjfuxEf6xVRxbUNO#scrollTo=6J-FYdx6nFE_), refined by [Anastasia Ianina](https://www.linkedin.com/in/anastasia-ianina/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use BERT implementation from `pytorch-transformers` library, which contains almost all recent architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "0NmMdkZO8R6q",
    "outputId": "1cc59bfa-1dbb-4540-cb22-196f399f62af"
   },
   "outputs": [],
   "source": [
    "# ! pip install pytorch-transformers\n",
    "# ! wget -O negative.csv 'https://www.dropbox.com/s/qwp22e0t3d3n2xa/negative.csv?dl=0'\n",
    "# ! wget -O positive.csv 'https://www.dropbox.com/s/t6nxxuplzsyica6/positive.csv?dl=0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ok002ceNB8E7",
    "outputId": "06ef90d2-7518-4209-da66-1dd45c357c78"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_transformers import BertTokenizer, BertConfig\n",
    "from pytorch_transformers import AdamW, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если у вас есть GPU, будем использовать ее для обучения. Тем не менее, этот ноутбук можно выполнить и с помощью только CPU. Правда, это будет значительно дольше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oYsV4H8fCpZ-",
    "outputId": "b8812c8e-3149-475f-b4c0-262160485c39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tesla P100-PCIE-16GB GPUs\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if device == torch.device('cpu'):\n",
    "    print('Using cpu')\n",
    "else:\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    print('Using {} GPUs'.format(torch.cuda.get_device_name(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "guw6ZNtaswKc"
   },
   "source": [
    "## Загрузка данных\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы выбрали не очень известный, необычный датасет с разметкой сентимента русскоязычных твитов (подробнее про него в [статье](http://www.swsys.ru/index.php?page=article&id=3962&lang=)). В корпусе, который мы использовали 114,911 положительных и 111,923 отрицательных записей. Загрузить его можно [тут](https://study.mokoron.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pos_texts = pd.read_csv('Data/positive.csv', encoding='utf8', sep=';', header=None)\n",
    "neg_texts = pd.read_csv('Data/negative.csv', encoding='utf8', sep=';', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87765</th>\n",
       "      <td>410816896171507713</td>\n",
       "      <td>1386781355</td>\n",
       "      <td>Elina12__</td>\n",
       "      <td>RT @ElUsmanova: после колледжа повалились в сн...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3083</td>\n",
       "      <td>630</td>\n",
       "      <td>93</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36039</th>\n",
       "      <td>409780683251281920</td>\n",
       "      <td>1386534302</td>\n",
       "      <td>zana_veska</td>\n",
       "      <td>@AlexandraPyshko ахаха ну да) как-то реальная ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1863</td>\n",
       "      <td>35</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30425</th>\n",
       "      <td>409641389442748416</td>\n",
       "      <td>1386501092</td>\n",
       "      <td>LogicOf_maggot</td>\n",
       "      <td>@Maggot_Vol_3 я меняю оформление слишком часто...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15497</td>\n",
       "      <td>185</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5955</th>\n",
       "      <td>409068126060707841</td>\n",
       "      <td>1386364416</td>\n",
       "      <td>susliknax</td>\n",
       "      <td>@Oleg_Amnell @shwoops я кроме этого еще хуйни ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16370</td>\n",
       "      <td>148</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57019</th>\n",
       "      <td>410120075979800576</td>\n",
       "      <td>1386615220</td>\n",
       "      <td>violentiyw</td>\n",
       "      <td>Все. Не могу больше этот бред смотреть. Даже к...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0           1               2   \\\n",
       "87765  410816896171507713  1386781355       Elina12__   \n",
       "36039  409780683251281920  1386534302      zana_veska   \n",
       "30425  409641389442748416  1386501092  LogicOf_maggot   \n",
       "5955   409068126060707841  1386364416       susliknax   \n",
       "57019  410120075979800576  1386615220      violentiyw   \n",
       "\n",
       "                                                      3   4   5   6   7   \\\n",
       "87765  RT @ElUsmanova: после колледжа повалились в сн...   1   0   1   0   \n",
       "36039  @AlexandraPyshko ахаха ну да) как-то реальная ...   1   0   0   0   \n",
       "30425  @Maggot_Vol_3 я меняю оформление слишком часто...   1   0   0   0   \n",
       "5955   @Oleg_Amnell @shwoops я кроме этого еще хуйни ...   1   0   0   0   \n",
       "57019  Все. Не могу больше этот бред смотреть. Даже к...   1   0   0   0   \n",
       "\n",
       "          8    9    10  11  \n",
       "87765   3083  630   93   2  \n",
       "36039   1863   35   24   0  \n",
       "30425  15497  185  128   4  \n",
       "5955   16370  148   76   1  \n",
       "57019     11    5    7   0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_texts.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на специальные токены [CLS] и [SEP], которые мы добавляем в началои конец предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.concatenate([pos_texts[3].values, neg_texts[3].values])\n",
    "\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = [[1] for _ in range(pos_texts.shape[0])] + [[0] for _ in range(neg_texts.shape[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(sentences) == len(labels) == pos_texts.shape[0] + neg_texts.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Дим, ты помогаешь мне, я тебе, все взаимно, все правильно) [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, test_sentences, train_gt, test_gt = train_test_split(sentences, labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158783 68051\n"
     ]
    }
   ],
   "source": [
    "print(len(train_gt), len(test_gt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ex5O1eV-Pfct"
   },
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BTREubVNFiz4"
   },
   "source": [
    "Теперь импортируем токенизатор для BERT'а, который превратит наши тексты в набор токенов, соответствующих тем, что встречаются в словаре предобученной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "Z474sSC6oe7A",
    "outputId": "fbaa8fd8-bccd-4feb-ce52-beba5d293cfa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 632084.35B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'н', '##а', '##м', 'с', 'е', '##г', '##о', '##р', '##ы', '##ч', '##е', '##м', 'п', '##р', '##о', '##с', '##т', '##о', 'н', '##е', '##ч', '##е', '##г', '##о', 'д', '##е', '##л', '##а', '##т', '##ь', 'у', 'б', '##а', '##б', '##у', '##ш', '##к', '##и', ':', '|', '#', 'web', '##cam', '##toy', 'http', ':', '/', '/', 't', '.', 'co', '/', 'q', '##5', '##aa', '##3', '##m', '##8', '##2', '##ee', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from pytorch_transformers import BertTokenizer, BertConfig\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in train_sentences]\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "87_kXUeT2-br"
   },
   "source": [
    "BERT'у нужно предоставить специальный формат входных данных.\n",
    "\n",
    "\n",
    "- **input ids**: последовательность чисел, отождествляющих каждый токен с его номером в словаре.\n",
    "- **labels**: вектор из нулей и единиц. В нашем случае нули обозначают негативную эмоциональную окраску, единицы - положительную.\n",
    "- **segment mask**: (необязательно) последовательность нулей и единиц, которая показывает, состоит ли входной текст из одного или двух предложений. Для случая одного предложения получится вектор из одних нулей. Для двух: <length_of_sent_1> нулей и <length_of_sent_2> единиц.\n",
    "- **attention mask**: (необязательно) последовательность нулей и единиц, где единицы обозначают токены предложения, нули - паддинг."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Паддинг нужен для того, чтобы BERT мог работать с предложениями разной длины. Выбираем максимально возможную длину предложения (в нашем случае пусть это будет 100). \n",
    "\n",
    "Теперь более длинные предложения будем обрезать до 100 токенов, а для более коротких использовать паддинг. Возьмем готовую функцию `pad_sequences` из библиотеки `keras`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cp9BPRd1tMIo"
   },
   "outputs": [],
   "source": [
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(\n",
    "    input_ids,\n",
    "    maxlen=100,\n",
    "    dtype=\"long\",\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\"\n",
    ")\n",
    "attention_masks = [[float(i>0) for i in seq] for seq in input_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делим данные на `train` и `val`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFbE-UHvsb7-"
   },
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n",
    "    input_ids, train_gt, \n",
    "    random_state=42,\n",
    "    test_size=0.1\n",
    ")\n",
    "\n",
    "train_masks, validation_masks, _, _ = train_test_split(\n",
    "    attention_masks,\n",
    "    input_ids,\n",
    "    random_state=42,\n",
    "    test_size=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем данные в `pytorch` тензоры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jw5K2A5Ko1RF"
   },
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [1],\n",
       "        [0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся классом `DataLoader`. Это поможет нам использовать эффективнее память во время тренировки модели, так как нам не нужно будет загружать в память весь датасет. Данные по батчам будем разбивать произвольно с помощью RandomSampler. Также обратите внимание на размер батча: если во время тренировки возникнет `Memory Error`, размер батча необходимо уменьшить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GEgLpFVlo1Z-"
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    sampler=RandomSampler(train_data),\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_data,\n",
    "    sampler=SequentialSampler(validation_data),\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pNl8khAhPYju"
   },
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь когда данные подготовлены, надо написать пайплайн обучения модели.\n",
    "\n",
    "Для начала мы хотим изменить предобученный BERT так, чтобы он выдавал метки для классификации текстов, а затем файнтюнить его на наших данных. Мы возьмем готовую модификацию BERTа для классификации из pytorch-transformers. Она интуитивно понятно называется `BertForSequenceClassification`. Это обычный BERT с добавленным линейным слоем для классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем [BertForSequenceClassification](https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/modeling.py#L1129):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_transformers import AdamW, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогичные модели есть и для других задач. Все они построены на основе одной и той же архитектуры и различаются только верхними слоями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_transformers import BertForQuestionAnswering, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь подробнее рассмотрим процесс файн-тюнинга. Как мы помним, первый токен в каждом предложении - это `[CLS]`. В отличие от скрытого состояния, относящего к обычному слову (не метке `[CLS]`), скрытое состояние относящееся к этой метке должно содержать в себе аггрегированное представление всего предложения, которое дальше будет использоваться для классификации. Таким образом, когда мы скормили предложение в процессе обучения сети, выходом будет вектор со скрытым состоянием, относящийся к метке `[CLS]`. Дополнительный полносвязный слой, который мы добавили, имеет размер `[hidden_state, количество_классов]`, в нашем случае количество классов равно двум. То есть нав выходе мы получим два числа, представляющих классы \"положительная эмоциональная окраска\" и \"отрицательная эмоциональная окраска\".\n",
    "\n",
    "Процесс дообучения достаточно дешев. По факту мы тренируем наш верхний слой и немного меняем веса во всех остальных слоях в процессе, чтобы подстроиться под нашу задачу.\n",
    "\n",
    "Иногда некоторые слои специально \"замораживают\" или применяют разные стратегии работы с learning rate, в общем, делают все, чтобы сохранить \"хорошие\" веса в нижних слоях и ускорить дообучение. В целом, замораживание слоев BERTа обычно не сильно сказывается на итоговом качестве, однако надо помнить о тех случаях, когда данные, использованные для предобучения и дообучения очень разные (разные домены или стиль: академическая и разговорная лексика). В таких случаях лучше тренировать все слои сети, не замораживая ничего."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WnQW9E-bBCRt"
   },
   "source": [
    "Загружаем BERT. `bert-base-uncased` - это версия \"base\" (в оригинальной статье рассказывается про две модели: \"base\" vs \"large\"), где есть только буквы в нижнем регистре (\"uncased\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gFsCTp_mporB",
    "outputId": "dd067229-1925-4b37-f517-0c14e25420d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:00<00:00, 94754.03B/s]\n",
      "100%|██████████| 440473133/440473133 [00:56<00:00, 7820654.56B/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обсудим гиперпараметры для обучения нашей модели. Авторы статьи советуют выбирать `learning rate` `5e-5`, `3e-5`, `2e-5`, а количество эпох не делать слишком большим, 2-4 вполне достаточно. Мы пойдем еще дальше и попробуем дообучить нашу модель всего за одну эпоху."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QxSMw0FrptiL"
   },
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "colab_type": "code",
    "id": "6J-FYdx6nFE_",
    "outputId": "8e388ad1-f9db-4c7b-d080-6c0a0e964610"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss на обучающей выборке: 0.03578\n",
      "Процент правильных предсказаний на валидационной выборке: 98.19%\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Будем сохранять loss во время обучения\n",
    "# и рисовать график в режиме реального времени\n",
    "train_loss_set = []\n",
    "train_loss = 0\n",
    "\n",
    "\n",
    "# Обучение\n",
    "# Переводим модель в training mode\n",
    "model.train()\n",
    "\n",
    "\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    # добавляем батч для вычисления на GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Распаковываем данные из dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # если не сделать .zero_grad(), градиенты будут накапливаться\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "    train_loss_set.append(loss[0].item())  \n",
    "    \n",
    "    # Backward pass\n",
    "    loss[0].backward()\n",
    "    \n",
    "    # Обновляем параметры и делаем шаг используя посчитанные градиенты\n",
    "    optimizer.step()\n",
    "\n",
    "    # Обновляем loss\n",
    "    train_loss += loss[0].item()\n",
    "    \n",
    "    # Рисуем график\n",
    "    clear_output(True)\n",
    "    plt.plot(train_loss_set)\n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"Batch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    \n",
    "print(\"Loss на обучающей выборке: {0:.5f}\".format(train_loss / len(train_dataloader)))\n",
    "\n",
    "\n",
    "# Валидация\n",
    "# Переводим модель в evaluation mode\n",
    "model.eval()\n",
    "\n",
    "valid_preds, valid_labels = [], []\n",
    "\n",
    "for batch in validation_dataloader:   \n",
    "    # добавляем батч для вычисления на GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    # Распаковываем данные из dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # При использовании .no_grad() модель не будет считать и хранить градиенты.\n",
    "    # Это ускорит процесс предсказания меток для валидационных данных.\n",
    "    with torch.no_grad():\n",
    "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "    # Перемещаем logits и метки классов на CPU для дальнейшей работы\n",
    "    logits = logits[0].detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    batch_preds = np.argmax(logits, axis=1)\n",
    "    batch_labels = np.concatenate(label_ids)     \n",
    "    valid_preds.extend(batch_preds)\n",
    "    valid_labels.extend(batch_labels)\n",
    "\n",
    "print(\"Процент правильных предсказаний на валидационной выборке: {0:.2f}%\".format(\n",
    "    accuracy_score(valid_labels, valid_preds) * 100\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент правильных предсказаний на валидационной выборке: 98.19%\n"
     ]
    }
   ],
   "source": [
    "print(\"Процент правильных предсказаний на валидационной выборке: {0:.2f}%\".format(\n",
    "    accuracy_score(valid_labels, valid_preds) * 100\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mkyubuJSOzg3"
   },
   "source": [
    "# Оценка качества на отложенной выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество на валидационной выборке оказалось очень хорошим. Не переобучилась ли наша модель?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем точно такую же предобработку для тестовых данных, как и в начале ноутбука делали для обучающих данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mAN0LZBOOPVh"
   },
   "outputs": [],
   "source": [
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in test_sentences]\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "input_ids = pad_sequences(\n",
    "    input_ids,\n",
    "    maxlen=100,\n",
    "    dtype=\"long\",\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем attention маски и приводим данные в необходимый формат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = [[float(i>0) for i in seq] for seq in input_ids]\n",
    "\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(test_gt)\n",
    "\n",
    "prediction_data = TensorDataset(\n",
    "    prediction_inputs,\n",
    "    prediction_masks,\n",
    "    prediction_labels\n",
    ")\n",
    "\n",
    "prediction_dataloader = DataLoader(\n",
    "    prediction_data, \n",
    "    sampler=SequentialSampler(prediction_data),\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hba10sXR7Xi6"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_preds, test_labels = [], []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "    # добавляем батч для вычисления на GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    # Распаковываем данные из dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # При использовании .no_grad() модель не будет считать и хранить градиенты.\n",
    "    # Это ускорит процесс предсказания меток для тестовых данных.\n",
    "    with torch.no_grad():\n",
    "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "    # Перемещаем logits и метки классов на CPU для дальнейшей работы\n",
    "    logits = logits[0].detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Сохраняем предсказанные классы и ground truth\n",
    "    batch_preds = np.argmax(logits, axis=1)\n",
    "    batch_labels = np.concatenate(label_ids)  \n",
    "    test_preds.extend(batch_preds)\n",
    "    test_labels.extend(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = accuracy_score(test_labels, test_preds)\n",
    "print('Процент правильных предсказаний на отложенной выборке составил: {0:.2f}%'.format(\n",
    "    acc_score*100\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Неправильных предсказаний: {0}/{1}'.format(\n",
    "    sum(test_labels != test_preds),\n",
    "    len(test_labels)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка качества работы без fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wo_finetuning = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model_wo_finetuning.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wo_finetuning.eval()\n",
    "preds_wo_finetuning, labels_wo_finetuning = [], []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    with torch.no_grad():\n",
    "        logits = model_wo_finetuning(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "    logits = logits[0].detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    batch_preds = np.argmax(logits, axis=1)\n",
    "    batch_labels = np.concatenate(label_ids)  \n",
    "    preds_wo_finetuning.extend(batch_preds)\n",
    "    labels_wo_finetuning.extend(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент правильных предсказаний на отложенной выборке составил: 48.57%\n"
     ]
    }
   ],
   "source": [
    "acc_score_wo_finetuning = accuracy_score(labels_wo_finetuning, preds_wo_finetuning)\n",
    "print('Процент правильных предсказаний на отложенной выборке составил: {0:.2f}%'.format(\n",
    "    acc_score_wo_finetuning*100\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним точность и полноту предсказаний:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 эпоха: точность (precision) 99.93%, полнота (recall) 96.34%\n",
      "Без дообучения: точность (precision) 37.68%, полнота (recall) 2.91%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score\n",
    "\n",
    "print('1 эпоха: точность (precision) {0:.2f}%, полнота (recall) {1:.2f}%'.format(\n",
    "    precision_score(test_labels, test_preds) * 100,\n",
    "    recall_score(test_labels, test_preds) * 100\n",
    "))\n",
    " \n",
    "print('Без дообучения: точность (precision) {0:.2f}%, полнота (recall) {1:.2f}%'.format(\n",
    "    precision_score(labels_wo_finetuning, preds_wo_finetuning) * 100,\n",
    "    recall_score(labels_wo_finetuning, preds_wo_finetuning) * 100,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы показали, что предобученный BERT может быстро (всего за одну эпоху) давать хорошее качество при решении задачи анализа эмоциональной окраски текстов. Обратите внимание, что мы не тюнили параметры и использовали сравнительно небольшой размеченный корпус, чтобы получить accuracy больше 98\\%. Тем не менее, если не делать дообучения под конкретную задачу вовсе, получить хорошее качество не удается.\n",
    "\n",
    "Кроме того, мы познакомились с библиотекой `pytorch-transformers`, которая позволяет использовать готовые обертки над моделями, специально созданными для решения той или иной задачи. Использовать BERT при решении повседневных NLP задач совсем нетрудно: не нужно даже вручную скачивать веса модели, библиотека все сделает за вас. Отбросив необходимость чуть-чуть предобработать тексты, сложность применения предобученного BERT'а оказывается не сильно больше, чем импортировать и применить лог.регрессию из `sklearn`."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT Fine-Tuning Sentence Classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
