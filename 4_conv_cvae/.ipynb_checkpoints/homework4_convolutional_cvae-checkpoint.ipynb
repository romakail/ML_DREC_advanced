{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook accompanies the [week15] practice. Refer to the classwork for more details.\n",
    "\n",
    "\n",
    "Your last and ultimate task is to implement and train __Convolutional Conditional VAE__. Simple VAE is available in week 15. For details about conditional VAE one can refer to [week 15 lecture](https://github.com/ml-mipt/ml-mipt/tree/advanced/week15_generative) or [this habr post (ru)](https://habr.com/ru/post/331664/)\n",
    "\n",
    "If it seems too easy, you can use [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset instead of MNIST.\n",
    "\n",
    "The code in general duplicates the one from the in-class practice. \n",
    "\n",
    "Do not forget to __use GPU acceleration during training__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Currently you are using device:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's dangerous to walk alone. Take these ;)\n",
    "\n",
    "class Rescale(object):\n",
    "    def __call__(self, image):\n",
    "        image = image - image.min()\n",
    "        image = image/image.max()\n",
    "\n",
    "        return image\n",
    "    \n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.flatten(x, start_dim=1, end_dim=-1)\n",
    "\n",
    "\n",
    "class RestoreShape(nn.Module):\n",
    "    def __init__(self, initial_shape):\n",
    "        super().__init__()\n",
    "        self.initial_shape = initial_shape\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view([-1]+list(self.initial_shape))\n",
    "    \n",
    "class MyPrint(nn.Module):\n",
    "    def __init__(self, layer_num):\n",
    "        self.layer_num = layer_num\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print ('[', self.layer_num, '] : ', x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data loading stuff is done for you ;)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transformations = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    Rescale()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "SHUFFLE_DATASET = True\n",
    "NUM_DATALOADER_WORKERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=torchvision.datasets.MNIST(\n",
    "        root=data_root,\n",
    "        train=True,\n",
    "        transform=mnist_transformations,\n",
    "        download=True\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=SHUFFLE_DATASET,\n",
    "    num_workers=NUM_DATALOADER_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=torchvision.datasets.MNIST(\n",
    "        root=data_root,\n",
    "        train=False,\n",
    "        transform=mnist_transformations\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_DATALOADER_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The code below is simple VAE. Your task is to make in convolutional (both encoder and decoder) and add class label information.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalCVAE(nn.Module):\n",
    "    def __init__(self, intermediate_dims, latent_dim, input_shape):\n",
    "        super().__init__()\n",
    "        self.register_buffer('_initial_mu', torch.zeros((latent_dim)))\n",
    "        self.register_buffer('_initial_sigma', torch.ones((latent_dim)))\n",
    "\n",
    "        self.latent_distribution = torch.distributions.normal.Normal(\n",
    "            loc=self._initial_mu,\n",
    "            scale=self._initial_sigma\n",
    "        )\n",
    "        input_dim = np.prod(input_shape)\n",
    "        self.encoder = nn.Sequential(*[\n",
    "            nn.Conv2d(1, intermediate_dims[0], 4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(intermediate_dims[0]),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(intermediate_dims[0], intermediate_dims[1], 5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(intermediate_dims[1]),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(intermediate_dims[1], intermediate_dims[2], 5),\n",
    "            Flatten(),\n",
    "        ])\n",
    "        \n",
    "        self.mu_repr = nn.Linear(\n",
    "            intermediate_dims[1],\n",
    "            latent_dim)\n",
    "        self.log_sigma_repr = nn.Linear(\n",
    "            intermediate_dims[1],\n",
    "            latent_dim) \n",
    "        \n",
    "        self.decoder = nn.Sequential(*[\n",
    "            nn.Linear(latent_dim, intermediate_dims[2]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(intermediate_dims[2]),\n",
    "            nn.Dropout(0.3),\n",
    "            RestoreShape([intermediate_dims[2], 1, 1]),\n",
    "            nn.ConvTranspose2d(intermediate_dims[2], intermediate_dims[1], 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(intermediate_dims[1]),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ConvTranspose2d(intermediate_dims[1], intermediate_dims[0], 5, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(intermediate_dims[0]),\n",
    "            nn.ConvTranspose2d(intermediate_dims[0], 1, 4, stride=2),\n",
    "            nn.Sigmoid()\n",
    "            \n",
    "            \n",
    "#             nn.Linear(latent_dim, intermediate_dims[1]),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.BatchNorm1d(intermediate_dims[1]),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(intermediate_dims[1], intermediate_dims[0]),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.BatchNorm1d(intermediate_dims[0]),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(intermediate_dims[0], input_dim),\n",
    "#             nn.Sigmoid(),\n",
    "#             RestoreShape(input_shape)\n",
    "        ])\n",
    "    \n",
    "    def _encode(self, x):\n",
    "        latent_repr = self.encoder(x)\n",
    "        mu_values = self.mu_repr(latent_repr)\n",
    "        log_sigma_values = self.log_sigma_repr(latent_repr)\n",
    "        return mu_values, log_sigma_values, latent_repr\n",
    "    \n",
    "    def _reparametrize(self, sample, mu_values, log_sigma_values):\n",
    "        latent_sample = sample * log_sigma_values.exp() + mu_values\n",
    "        # <YOUR CODE HERE>\n",
    "        return latent_sample\n",
    "\n",
    "    def forward(self, x, raw_sample=None):\n",
    "        mu_values, log_sigma_values, latent_repr = self._encode(x)\n",
    "\n",
    "        if raw_sample is None:\n",
    "            raw_sample = torch.randn_like(mu_values)\n",
    "\n",
    "        latent_sample = self._reparametrize(raw_sample, mu_values, log_sigma_values)\n",
    "        \n",
    "        reconstructed_repr = self.decoder(latent_sample)\n",
    "        \n",
    "        return reconstructed_repr, latent_sample, mu_values, log_sigma_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(*args):\n",
    "    args = [x.squeeze() for x in args]\n",
    "    n = min([x.shape[0] for x in args])\n",
    "    \n",
    "    fig = plt.figure(figsize=(2*n, 2*len(args)))\n",
    "    for j in range(n):\n",
    "        for i in range(len(args)):\n",
    "            ax = plt.subplot(len(args), n, i*n + j + 1)\n",
    "            plt.imshow(args[i][j])\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "\n",
    "#     plt.show()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (30) : unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:50",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-49f60d58951f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexample_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexample_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexample_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mexample_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvolutionalCVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-49f60d58951f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexample_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexample_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexample_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mexample_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvolutionalCVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m    178\u001b[0m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0m_cudart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudaGetErrorName\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (30) : unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:50"
     ]
    }
   ],
   "source": [
    "example_batch = next(iter(train_loader))\n",
    "example_batch = [x.to(device) for x in example_batch]\n",
    "example_x = example_batch[0][0]\n",
    "\n",
    "model = ConvolutionalCVAE([256, 128, 128], 2, example_x.shape).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_func = torch.nn.modules.loss.BCELoss()\n",
    "\n",
    "print (example_batch[0][:15].to(device).shape)\n",
    "reconstructed_repr, latent_sample, mu_values, log_sigma_values = model(example_batch[0][:15].to(device))\n",
    "\n",
    "summary(model, example_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_loss = 0.5 * torch.mean(torch.sum(\n",
    "    mu_values.pow(2) + torch.exp(log_sigma_values) - 1. - log_sigma_values,\n",
    "    dim=1\n",
    "))# <YOUR CODE HERE>\n",
    "kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_predictions(model, test_loader):\n",
    "    model.eval()\n",
    "    reconstructed_repr_list, latent_samples_list, mu_values_list, log_sigma_values_list = [], [], [], []\n",
    "    for test_batch in tqdm.tqdm_notebook(test_loader, leave=False):\n",
    "        out = model(test_batch[0].to(device))\n",
    "        reconstructed_repr, latent_sample, mu_values, log_sigma_values = [x.detach().cpu() for x in out]\n",
    "        reconstructed_repr_list.append(reconstructed_repr)\n",
    "        latent_samples_list.append(latent_sample)\n",
    "        mu_values_list.append(mu_values)\n",
    "        log_sigma_values_list.append(log_sigma_values)\n",
    "    return [\n",
    "        torch.cat(_list, dim=0)\n",
    "        for _list in [reconstructed_repr_list, latent_samples_list, mu_values_list, log_sigma_values_list]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_repr, latent_sample, mu_values, log_sigma_values = get_test_predictions(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15 # to generate image with 15x15 examples\n",
    "digit_size = 28\n",
    "latent_dim = 2\n",
    "from scipy.stats import norm\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "def draw_manifold(model, show=True):\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    for i, yi in enumerate(grid_x):\n",
    "        for j, xi in enumerate(grid_y):\n",
    "            z_sample = np.zeros((1, latent_dim))\n",
    "            z_sample[:, :2] = np.array([[xi, yi]])\n",
    "            \n",
    "            z_torch = torch.from_numpy(z_sample).type(torch.FloatTensor).to(device)\n",
    "            \n",
    "            x_decoded = model.decoder(z_torch).detach().cpu().numpy()\n",
    "            digit = x_decoded[0].squeeze()\n",
    "            figure[i * digit_size: (i + 1) * digit_size,\n",
    "                   j * digit_size: (j + 1) * digit_size] = digit\n",
    "    if show:\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        plt.imshow(figure, cmap='Greys_r')\n",
    "#         plt.grid(None)\n",
    "        ax = plt.gca()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)        \n",
    "        plt.show()\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "_img = draw_manifold(model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch):\n",
    "    # Saving manifold and z distribution to build plots and animation afterwards\n",
    "    figure = draw_manifold(model, show=False)\n",
    "    reconstructed_repr, latent_sample, mu_values, log_sigma_values = get_test_predictions(model, test_loader)\n",
    "    return figure, reconstructed_repr, latent_sample, mu_values, log_sigma_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    epochs, figs, latent_distrs = [], [], []\n",
    "    for epoch_num in tqdm.tnrange(num_epochs):\n",
    "        model.train()\n",
    "        loss_accumulator = 0.\n",
    "        bce_acc = 0.\n",
    "        kl_acc = 0.\n",
    "        for batch_x, batch_label in tqdm.tqdm_notebook(train_loader, leave=False):\n",
    "            batch_x = batch_x.to(device)\n",
    "            predictions, latent, mu_values, log_sigma_values = model(batch_x)\n",
    "\n",
    "            kl_loss = 0.5 * torch.mean(torch.sum(\n",
    "                mu_values.pow(2) + torch.exp(log_sigma_values) - 1. - log_sigma_values,\n",
    "                dim=1\n",
    "            ))# <YOUR CODE HERE>\n",
    "            bce_loss = 28*28*loss_func(predictions, batch_x)\n",
    "            loss = (bce_loss + kl_loss)/2./28./28.\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            loss_accumulator += loss/(len(train_loader.dataset))\n",
    "            bce_acc += bce_loss/(len(train_loader.dataset))\n",
    "            kl_acc += kl_loss/(len(train_loader.dataset))\n",
    "\n",
    "        if epoch_num % 5 == 0:\n",
    "            print('Epoch num: {}\\nTraining loss={:.4f}, KL divergence={:.4f}, BCE Loss={:.4f}'.format(\n",
    "                epoch_num,\n",
    "                loss_accumulator,\n",
    "                kl_acc,\n",
    "                bce_acc\n",
    "            ))\n",
    "            model.eval()\n",
    "            figure, reconstructed_repr_test, latent_sample_test, mu_values_test, log_sigma_values_test = on_epoch_end(epoch_num)\n",
    "            epochs.append(epoch_num)\n",
    "            figs.append(figure)\n",
    "            latent_distrs.append((mu_values_test, log_sigma_values_test))\n",
    "    return epochs, figs, latent_distrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs, figs, latent_distrs = train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "for b in test_loader:\n",
    "    test_labels.append(b[1])\n",
    "test_labels = torch.cat(test_labels, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('my_figs', exist_ok=True)\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib import cm\n",
    "import matplotlib\n",
    "\n",
    "def make_2d_figs_gif(figs, epochs, fname, fig): \n",
    "    norm = matplotlib.colors.Normalize(vmin=0, vmax=1, clip=False)\n",
    "    im = plt.imshow(np.zeros((28,28)), cmap='Greys_r', norm=norm)\n",
    "    plt.grid(None)\n",
    "    plt.title(\"Epoch: \" + str(epochs[0]))\n",
    "\n",
    "    def update(i):\n",
    "        im.set_array(figs[i])\n",
    "        im.axes.set_title(\"Epoch: \" + str(epochs[i]))\n",
    "        im.axes.get_xaxis().set_visible(False)\n",
    "        im.axes.get_yaxis().set_visible(False)\n",
    "        return im\n",
    "    \n",
    "    anim = FuncAnimation(fig, update, frames=range(len(figs)), interval=100)\n",
    "    anim.save(fname, dpi=80, writer='imagemagick')\n",
    "\n",
    "def make_2d_scatter_gif(zs, epochs, c, fname, fig):\n",
    "    im = plt.scatter(zs[0][:, 0], zs[0][:, 1], c=c, cmap=cm.coolwarm)\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Epoch: \" + str(epochs[0]))\n",
    "    def update(i):\n",
    "        fig.clear()\n",
    "        im = plt.scatter(zs[i][:, 0], zs[i][:, 1], c=c, cmap=cm.coolwarm)\n",
    "        im.axes.set_title(\"Epoch: \" + str(epochs[i]))\n",
    "        im.axes.set_xlim(-5, 5)\n",
    "        im.axes.set_ylim(-5, 5)\n",
    "        return im\n",
    "\n",
    "    anim = FuncAnimation(fig, update, frames=range(len(zs)), interval=150)\n",
    "    anim.save(fname, dpi=80, writer='imagemagick')\n",
    "    \n",
    "make_2d_figs_gif(figs, epochs, \"./my_figs/manifold2.gif\", plt.figure(figsize=(10,10)))\n",
    "make_2d_scatter_gif([x[0].numpy() for x in latent_distrs], epochs, test_labels, \"./my_figs/z_distr2.gif\", plt.figure(figsize=(10,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find your brand gifs in `./my_figs/` directory ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally you can also implement GAN for this task. Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
