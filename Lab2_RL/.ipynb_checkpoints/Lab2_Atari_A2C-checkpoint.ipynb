{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Mastering A2C (and Kung-Fu)\n",
    "\n",
    "This part is based on [Practical RL week08 practice](https://github.com/yandexdataschool/Practical_RL/tree/master/week08_pomdp). All rights belong to original authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Please, note that supplementary files require tensorflow version 1.15.0 (while TF 2.0 is actually released now). Please downgrade your TF or create new environment.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This versions shpuld work fine\n",
    "\n",
    "# ! conda install scipy==1.0.1\n",
    "# ! conda install tensorflow==1.15.0 --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are displaying\n",
      "Starting virtual X frame buffer: Xvfbstart-stop-daemon: unable to stat /usr/bin/Xvfb (No such file or directory)\r\n",
      ".\r\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from IPython.core import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "# if you're running in colab\n",
    "# !wget https://raw.githubusercontent.com/yandexdataschool/Practical_RL/0ccb0673965dd650d9b284e1ec90c2bfd82c8a94/week08_pomdp/atari_util.py\n",
    "# !wget https://raw.githubusercontent.com/yandexdataschool/Practical_RL/0ccb0673965dd650d9b284e1ec90c2bfd82c8a94/week08_pomdp/env_pool.py\n",
    "\n",
    "# If you are running on a server, launch xvfb to record game videos\n",
    "# Please make sure you have xvfb installed\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    print ('We are displaying')\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kung-Fu, recurrent style\n",
    "\n",
    "In this notebook we'll once again train RL agent for for atari [KungFuMaster](https://gym.openai.com/envs/KungFuMaster-v0/), this time using recurrent neural networks.\n",
    "\n",
    "![http://www.retroland.com/wp-content/uploads/2011/07/King-Fu-Master.jpg](http://www.retroland.com/wp-content/uploads/2011/07/King-Fu-Master.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape: (1, 42, 42)\n",
      "Num actions: 14\n",
      "Action names: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from atari_util import PreprocessAtari\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"KungFuMasterDeterministic-v0\")\n",
    "    env = PreprocessAtari(env, height=42, width=42,\n",
    "                          crop=lambda img: img[60:-30, 15:],\n",
    "                          color=False, n_frames=1)\n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation shape:\", obs_shape)\n",
    "print(\"Num actions:\", n_actions)\n",
    "print(\"Action names:\", env.env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "for _ in range(100):\n",
    "    s, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.title('Game image')\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.show()\n",
    "\n",
    "plt.title('Agent observation')\n",
    "plt.imshow(s.reshape([42, 42]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POMDP setting\n",
    "\n",
    "The atari game we're working with is actually a POMDP: your agent needs to know timing at which enemies spawn and move, but cannot do so unless it has some memory. \n",
    "\n",
    "Let's design another agent that has a recurrent neural net memory to solve this. Here's a sketch.\n",
    "\n",
    "![img](img1.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# a special module that converts [batch, channel, w, h] to [batch, units]\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRecurrentAgent(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions, reuse=False):\n",
    "        \"\"\"A simple actor-critic agent\"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "\n",
    "        self.conv0 = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.conv1 = nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.activation = nn.ReLU()\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        self.hid = nn.Linear(512, 128)\n",
    "        self.rnn = nn.LSTMCell(128, 128)\n",
    "\n",
    "        self.logits = nn.Linear(128, n_actions)\n",
    "        self.state_value = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, prev_state, obs_t):\n",
    "        \"\"\"\n",
    "        Takes agent's previous step and observation, \n",
    "        returns next state and whatever it needs to learn (tf tensors)\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE: apply the whole neural net for one step here.\n",
    "        # See docs on self.rnn(...)\n",
    "        # the recurrent cell should take the last feedforward dense layer as input\n",
    "        \n",
    "        \n",
    "        emb = self.activation(self.conv0(obs_t))\n",
    "        emb = self.activation(self.conv1(emb))\n",
    "        emb = self.activation(self.conv2(emb))\n",
    "        \n",
    "        emb = self.hid(self.flatten(emb))\n",
    "        \n",
    "        new_state = self.rnn(emb, prev_state)\n",
    "        logits = self.logits(new_state[0])\n",
    "        state_value = self.state_value(new_state[0])\n",
    "\n",
    "        return new_state, (logits, state_value)\n",
    "\n",
    "    def get_initial_state(self, batch_size):\n",
    "        \"\"\"Return a list of agent memory states at game start. Each state is a np array of shape [batch_size, ...]\"\"\"\n",
    "        return torch.zeros((batch_size, 128)), torch.zeros((batch_size, 128))\n",
    "\n",
    "    def sample_actions(self, agent_outputs):\n",
    "        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"\n",
    "        logits, state_values = agent_outputs\n",
    "        probs = F.softmax(logits)\n",
    "        return torch.multinomial(probs, 1)[:, 0].data.numpy()\n",
    "\n",
    "    def step(self, prev_state, obs_t):\n",
    "        \"\"\" like forward, but obs_t is a numpy array \"\"\"\n",
    "        obs_t = torch.tensor(np.asarray(obs_t), dtype=torch.float32)\n",
    "        (h, c), (l, s) = self.forward(prev_state, obs_t)\n",
    "        return (h.detach(), c.detach()), (l.detach(), s.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parallel_games = 5\n",
    "gamma = 0.99\n",
    "\n",
    "agent = SimpleRecurrentAgent(obs_shape, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action logits:\n",
      " tensor([[-0.0075, -0.0770, -0.0344,  0.0095, -0.0760,  0.0668,  0.0795, -0.0963,\n",
      "          0.0141,  0.0142,  0.0728,  0.0492, -0.0208, -0.0211]])\n",
      "state values:\n",
      " tensor([[0.0308]])\n"
     ]
    }
   ],
   "source": [
    "state = [env.reset()]\n",
    "_, (logits, value) = agent.step(agent.get_initial_state(1), state)\n",
    "print(\"action logits:\\n\", logits)\n",
    "print(\"state values:\\n\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play!\n",
    "Let's build a function that measures agent's average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_games=1):\n",
    "    \"\"\"Plays an entire game start to end, returns session rewards.\"\"\"\n",
    "\n",
    "    game_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        # initial observation and memory\n",
    "        observation = env.reset()\n",
    "        prev_memories = agent.get_initial_state(1)\n",
    "\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            new_memories, readouts = agent.step(\n",
    "                prev_memories, observation[None, ...])\n",
    "            action = agent.sample_actions(readouts)\n",
    "\n",
    "            observation, reward, done, info = env.step(action[0])\n",
    "\n",
    "            total_reward += reward\n",
    "            prev_memories = new_memories\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        game_rewards.append(total_reward)\n",
    "    return game_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3fe6c840f544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv_monitor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"kungfu_videos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_monitor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_games\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0menv_monitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-dbd96891fb54>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(agent, env, n_games)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_games\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# initial observation and memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mprev_memories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_after_reset\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         )\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_ansi_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_image_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m_encode_image_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode_image_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes_per_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoder_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_path, frame_shape, frames_per_sec)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ffmpeg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m: Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`."
     ]
    }
   ],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "rw = evaluate(agent, env_monitor, n_games=3,)\n",
    "env_monitor.close()\n",
    "print(rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s: s.endswith(\n",
    "    \".mp4\"), os.listdir(\"./kungfu_videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on parallel games\n",
    "\n",
    "We introduce a class called EnvPool - it's a tool that handles multiple environments for you. Here's how it works:\n",
    "![img](img2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_pool import EnvPool\n",
    "pool = EnvPool(agent, make_env, n_parallel_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gonna train our agent on a thing called __rollouts:__\n",
    "![img](img3.jpg)\n",
    "\n",
    "A rollout is just a sequence of T observations, actions and rewards that agent took consequently.\n",
    "* First __s0__ is not necessarily initial state for the environment\n",
    "* Final state is not necessarily terminal\n",
    "* We sample several parallel rollouts for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of n_parallel_games, take 10 steps\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actions shape:\", rollout_actions.shape)\n",
    "print(\"Rewards shape:\", rollout_rewards.shape)\n",
    "print(\"Mask shape:\", rollout_mask.shape)\n",
    "print(\"Observations shape: \", rollout_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic objective\n",
    "\n",
    "Here we define a loss function that uses rollout above to train advantage actor-critic agent.\n",
    "\n",
    "\n",
    "Our loss consists of three components:\n",
    "\n",
    "* __The policy \"loss\"__\n",
    " $$ \\hat J = {1 \\over T} \\cdot \\sum_t { \\log \\pi(a_t | s_t) } \\cdot A_{const}(s,a) $$\n",
    "  * This function has no meaning in and of itself, but it was built such that\n",
    "  * $ \\nabla \\hat J = {1 \\over N} \\cdot \\sum_t { \\nabla \\log \\pi(a_t | s_t) } \\cdot A(s,a) \\approx \\nabla E_{s, a \\sim \\pi} R(s,a) $\n",
    "  * Therefore if we __maximize__ J_hat with gradient descent we will maximize expected reward\n",
    "  \n",
    "  \n",
    "* __The value \"loss\"__\n",
    "  $$ L_{td} = {1 \\over T} \\cdot \\sum_t { [r + \\gamma \\cdot V_{const}(s_{t+1}) - V(s_t)] ^ 2 }$$\n",
    "  * Ye Olde TD_loss from q-learning and alike\n",
    "  * If we minimize this loss, V(s) will converge to $V_\\pi(s) = E_{a \\sim \\pi(a | s)} R(s,a) $\n",
    "\n",
    "\n",
    "* __Entropy Regularizer__\n",
    "  $$ H = - {1 \\over T} \\sum_t \\sum_a {\\pi(a|s_t) \\cdot \\log \\pi (a|s_t)}$$\n",
    "  * If we __maximize__ entropy we discourage agent from predicting zero probability to actions\n",
    "  prematurely (a.k.a. exploration)\n",
    "  \n",
    "  \n",
    "So we optimize a linear combination of $L_{td}$ $- \\hat J$, $-H$\n",
    "  \n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "__One more thing:__ since we train on T-step rollouts, we can use N-step formula for advantage for free:\n",
    "  * At the last step, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot V(s_{t+1}) - V(s) $\n",
    "  * One step earlier, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot r(s_{t+1}, a_{t+1}) + \\gamma ^ 2 \\cdot V(s_{t+2}) - V(s) $\n",
    "  * Et cetera, et cetera. This way agent starts training much faster since it's estimate of A(s,a) depends less on his (imperfect) value function and more on actual rewards. There's also a [nice generalization](https://arxiv.org/abs/1506.02438) of this.\n",
    "\n",
    "\n",
    "__Note:__ it's also a good idea to scale rollout_len up to learn longer sequences. You may wish set it to >=20 or to start at 10 and then scale up as time passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take an integer tensor and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y.to(dtype=torch.int64).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-932dae6a192b>, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-932dae6a192b>\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    memory, (logits_t, values_t) = <YOUR CODE >\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "def train_on_rollout(states, actions, rewards, is_not_done, prev_memory_states, gamma=0.99, device=device, max_grad_norm=90):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # shape: [batch_size, time, c, h, w]\n",
    "    states = torch.tensor(np.asarray(states), dtype=torch.float32)\n",
    "    actions = torch.tensor(np.array(actions), dtype=torch.int64)  # shape: [batch_size, time]\n",
    "    rewards = torch.tensor(np.array(rewards), dtype=torch.float32)  # shape: [batch_size, time]\n",
    "    is_not_done = torch.tensor(np.array(is_not_done), dtype=torch.float32)  # shape: [batch_size, time]\n",
    "    rollout_length = rewards.shape[1] - 1\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    memory = [m.detach() for m in prev_memory_states]\n",
    "\n",
    "    logits = []  # append logit sequence here\n",
    "    state_values = []  # append state values here\n",
    "    for t in range(rewards.shape[1]):\n",
    "        obs_t = states[:, t]\n",
    "\n",
    "        # use agent to comute logits_t and state values_t.\n",
    "        # append them to logits and state_values array\n",
    "\n",
    "        memory, (logits_t, values_t) = <YOUR CODE >\n",
    "\n",
    "        logits.append(logits_t)\n",
    "        state_values.append(values_t)\n",
    "\n",
    "    logits = torch.stack(logits, dim=1)\n",
    "    state_values = torch.stack(state_values, dim=1)\n",
    "    probas = F.softmax(logits, dim=2)\n",
    "    logprobas = F.log_softmax(logits, dim=2)\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    actions_one_hot = to_one_hot(actions, n_actions).view(\n",
    "        actions.shape[0], actions.shape[1], n_actions)\n",
    "    logprobas_for_actions = torch.sum(logprobas * actions_one_hot, dim=-1)\n",
    "\n",
    "    # Now let's compute two loss components:\n",
    "    # 1) Policy gradient objective.\n",
    "    # Notes: Please don't forget to call .detach() on advantage term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    J_hat = 0  # policy objective as in the formula for J_hat\n",
    "\n",
    "    # 2) Temporal difference MSE for state values\n",
    "    # Notes: Please don't forget to call on V(s') term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    value_loss = 0\n",
    "\n",
    "    cumulative_returns = state_values[:, -1].detach()\n",
    "\n",
    "    for t in reversed(range(rollout_length)):\n",
    "        r_t = rewards[:, t]                                # current rewards\n",
    "        # current state values\n",
    "        V_t = state_values[:, t]\n",
    "        V_next = state_values[:, t + 1].detach()           # next state values\n",
    "        # log-probability of a_t in s_t\n",
    "        logpi_a_s_t = logprobas_for_actions[:, t]\n",
    "\n",
    "        # update G_t = r_t + gamma * G_{t+1} as we did in week6 reinforce\n",
    "        cumulative_returns = G_t = r_t + gamma * cumulative_returns\n",
    "\n",
    "        # Compute temporal difference error (MSE for V(s))\n",
    "        value_loss += (r_t + gamma * V_next - V_t) ** 2#<YOUR CODE >\n",
    "\n",
    "        # compute advantage A(s_t, a_t) using cumulative returns and V(s_t) as baseline\n",
    "        advantage = r_t + gamma * V_next - V_t #<YOUR CODE >\n",
    "        advantage = advantage.detach()\n",
    "\n",
    "        # compute policy pseudo-loss aka -J_hat.\n",
    "        J_hat += logpi_a_s_t * advantage  #<YOUR CODE >\n",
    "\n",
    "    # regularize with entropy\n",
    "    entropy_reg = torch.mean(logprobas * probas)#<compute entropy regularizer >\n",
    "\n",
    "    # add-up three loss components and average over time\n",
    "    loss = -J_hat / rollout_length +\\\n",
    "        value_loss / rollout_length +\\\n",
    "           -0.01 * entropy_reg\n",
    "\n",
    "    # Gradient descent step\n",
    "    loss.backward()\n",
    "    # This small trick allows to clip gradients and to monitor them over the time\n",
    "    grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "    < your code >\n",
    "\n",
    "    return loss.data.numpy(), grad_norm, entropy_reg.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's test it\n",
    "memory = list(pool.prev_memory_states)\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)\n",
    "\n",
    "train_on_rollout(rollout_obs, rollout_actions,\n",
    "                 rollout_rewards, rollout_mask, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train \n",
    "\n",
    "just run train step and see if agent learns any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import trange\n",
    "from pandas import DataFrame\n",
    "moving_average = lambda x, **kw: DataFrame(\n",
    "    {'x': np.asarray(x)}).x.ewm(**kw).mean().values\n",
    "\n",
    "rewards_history = []\n",
    "grad_norm_history = []\n",
    "entropy_history = []\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15000, 25000):\n",
    "\n",
    "    memory = list(pool.prev_memory_states)\n",
    "    rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(\n",
    "        20)\n",
    "    loss, grad_norm, entropy = train_on_rollout(rollout_obs, rollout_actions,\n",
    "                     rollout_rewards, rollout_mask, memory)\n",
    "    grad_norm_history.append(grad_norm)\n",
    "    entropy_history.append(entropy)\n",
    "    loss_history.append(loss)\n",
    "    if i % 100 == 0:\n",
    "        rewards_history.append(np.mean(evaluate(agent, env, n_games=1)))\n",
    "        clear_output(True)\n",
    "        \n",
    "        plt.figure(figsize=[16, 9])\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.title(\"Mean reward\")\n",
    "        plt.plot(rewards_history, label='rewards')\n",
    "        plt.plot(moving_average(np.array(rewards_history),\n",
    "                                span=10), label='rewards ewma@10')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.title(\"Grad norm history (smoothened)\")\n",
    "        plt.plot(moving_average(np.array(grad_norm_history), span=100), label='grad norm ewma@100')\n",
    "        plt.grid()\n",
    "        \n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.title(\"entropy (smoothened)\")\n",
    "        plt.plot(moving_average(np.array(entropy_history), span=100), label='entropy ewma@100')\n",
    "        plt.grid()\n",
    "        \n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.title(\"loss (smoothened)\")\n",
    "        plt.plot(np.array(loss_history), label='loss raw')\n",
    "        plt.plot(moving_average(np.array(loss_history), span=10), label='loss ewma@10')\n",
    "        plt.grid()\n",
    "\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        if rewards_history[-1] >= 10000:\n",
    "            print(\"Your agent has just passed the minimum homework threshold\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relax and grab some refreshments while your agent is locked in an infinite loop of violence and death.\n",
    "\n",
    "__How to interpret plots:__\n",
    "\n",
    "The session reward is the easy thing: it should in general go up over time, but it's okay if it fluctuates ~~like crazy~~. It's also OK if it reward doesn't increase substantially before some 10k initial steps. However, if reward reaches zero and doesn't seem to get up over 2-3 evaluations, there's something wrong happening.\n",
    "\n",
    "\n",
    "Since we use a policy-based method, we also keep track of __policy entropy__ - the same one you used as a regularizer. The only important thing about it is that your entropy shouldn't drop too low (`< 0.1`) before your agent gets the yellow belt. Or at least it can drop there, but _it shouldn't stay there for long_.\n",
    "\n",
    "If it does, the culprit is likely:\n",
    "* Some bug in entropy computation. Remember that it is $ - \\sum p(a_i) \\cdot log p(a_i) $\n",
    "* Your agent architecture converges too fast. Increase entropy coefficient in actor loss. \n",
    "* Gradient explosion - just [clip gradients](https://stackoverflow.com/a/43486487) and maybe use a smaller network\n",
    "* Us. Or TF developers. Or aliens. Or lizardfolk. Contact us on forums before it's too late!\n",
    "\n",
    "If you're debugging, just run `logits, values = agent.step(batch_states)` and manually look into logits and values. This will reveal the problem 9 times out of 10: you'll likely see some NaNs or insanely large numbers or zeros. Try to catch the moment when this happens for the first time and investigate from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Final\" evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "final_rewards = evaluate(agent, env_monitor, n_games=20,)\n",
    "env_monitor.close()\n",
    "print(\"Final mean reward\", np.mean(final_rewards))\n",
    "\n",
    "video_names = list(filter(lambda s: s.endswith(\n",
    "    \".mp4\"), os.listdir(\"./kungfu_videos/\")))\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
