{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Mastering A2C (and Kung-Fu)\n",
    "\n",
    "This part is based on [Practical RL week08 practice](https://github.com/yandexdataschool/Practical_RL/tree/master/week08_pomdp). All rights belong to original authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Please, note that supplementary files require tensorflow version 1.15.0 (while TF 2.0 is actually released now). Please downgrade your TF or create new environment.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This versions shpuld work fine\n",
    "\n",
    "# ! conda install scipy==1.0.1\n",
    "# ! conda install tensorflow==1.15.0 --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are displaying\n",
      "bash: ../xvfb: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from IPython.core import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "# if you're running in colab\n",
    "# !wget https://raw.githubusercontent.com/yandexdataschool/Practical_RL/0ccb0673965dd650d9b284e1ec90c2bfd82c8a94/week08_pomdp/atari_util.py\n",
    "# !wget https://raw.githubusercontent.com/yandexdataschool/Practical_RL/0ccb0673965dd650d9b284e1ec90c2bfd82c8a94/week08_pomdp/env_pool.py\n",
    "\n",
    "# If you are running on a server, launch xvfb to record game videos\n",
    "# Please make sure you have xvfb installed\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    print ('We are displaying')\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kung-Fu, recurrent style\n",
    "\n",
    "In this notebook we'll once again train RL agent for for atari [KungFuMaster](https://gym.openai.com/envs/KungFuMaster-v0/), this time using recurrent neural networks.\n",
    "\n",
    "![http://www.retroland.com/wp-content/uploads/2011/07/King-Fu-Master.jpg](http://www.retroland.com/wp-content/uploads/2011/07/King-Fu-Master.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape: (1, 42, 42)\n",
      "Num actions: 14\n",
      "Action names: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from atari_util import PreprocessAtari\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"KungFuMasterDeterministic-v0\")\n",
    "    env = PreprocessAtari(env, height=42, width=42,\n",
    "                          crop=lambda img: img[60:-30, 15:],\n",
    "                          color=False, n_frames=1)\n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation shape:\", obs_shape)\n",
    "print(\"Num actions:\", n_actions)\n",
    "print(\"Action names:\", env.env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM4AAAEICAYAAAAX2cvZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAWG0lEQVR4nO3de9QcdX3H8feHIGIBIVxCuBM4yPHBS4yIaSkoeAspFmirBovipSVU0kJDT0lAI8YLAeUSSwWCUi5ikIoo9YQoBbz0IMjFEC4RCBchJORBbkG5tMRv/5jZMNnsPs8+s7vP7Mx+Xufs2dnfzO58J9nv85v5zex3FBGY2chsVHQAZmXkxDHLwYljloMTxywHJ45ZDk4csxycOBUkaVdJv5c0puhYqsqJ0wZJ0yTdIukPkgbT6c9IUpFxRcSjEbF5RKwtMo4qc+LkJOlEYD7wVWA8sD1wLLA/sEmBodloiAg/RvgAtgT+APz1MMv9BfBrYA3wGHBqZt7uQACfTOc9Q5J47wCWAs8C59Z93qeAZemyPwZ2a7Le2mdvnL7+KfAl4Cbg98B/AdsAl6ex3Qrsnnn//DSmNcDtwAGZea8DLkljWAb8K7AiM39H4CrgSeBh4J+K/v/qyneg6ADK+ACmAK/UvphDLPdu4M0kPftbgNXA4em82pf7fGBT4P3AS8APgHHATsAg8K50+cOB5cAbgY2BzwI3NVlvo8RZDuyZJv29wP3Ae9PPuhT4j8z7j0oTa2PgROAJYNN03jzgZ8BYYOc0yVek8zZKE20OSa+7B/AQ8IGi/886/h0oOoAyPtIv1hN1bTelvcSLwIFN3ncOcHY6Xfty75SZ/xTwkczrq4AT0ulrgU9n5m0EvECDXqdJ4pySmX8mcG3m9QeBJUNs7zPAW9Pp9RIB+LtM4rwTeLTuvbOzSVmVh49x8nkK2FbSxrWGiPiziNgqnbcRgKR3SrpR0pOSniPZFdu27rNWZ6ZfbPB683R6N2C+pGclPQs8DYikZ2pFq+tB0omSlkl6Ll3Xlpm4dyTZjavJTu8G7FiLMX3vySTHf5XixMnnl8DLwGHDLPcd4Bpgl4jYkmS3LO+I22PA9IjYKvN4XUTclPPzGpJ0AHAS8GFgbPrH4DlejXsVyS5azS51MT5cF+MWETG1kzH2AidODhHxLPAF4BuS/kbS5pI2kjQR2Cyz6BbA0xHxkqT9gI+2sdrzgdmS9gGQtKWkD7Xxec1sQXL89iSwsaQ5wOsz869M4xgraSdgRmber4A1kk6S9DpJYyS9SdI7uhBnoZw4OUXEGcBMklGlQZJdnwtI/lrXeoHPAHMlPU9ywHxlG+u7GjgduELSGuBu4JDcG9Dcj0mOp+4HfksyYJHdHZsLrCAZMftv4HskvS+RnDf6IDAxnf874Jsku3qVovQAziwXSf8ATIuIdxUdy2hyj2MjImkHSfunu6Z7kwxXX110XKNt4+EXMVvPJiS7pBNIht+vAL5RaEQF6NqumqQpJGegxwDfjIh5XVmRWQG6kjjpVbn3A+8jOZC8FTgyIu7t+MrMCtCtXbX9gOUR8RCApCtIznk0TBxJHqGwXvS7iNiu0YxuDQ7sxPpDmCuoO8Mt6RhJt0m6rUsxmLXrt81mdKvHaXR2fL1eJSIWAAvAPY6VT7d6nBWsfynGzsDKLq3LbNR1K3FuBfaSNEHSJsA0kmu2zCqhK7tqEfGKpBkkl2+MAS6KiHu6sS6zIvTEJTc+xrEedXtE7Ntohi+5McuhFJfcHH/88UWHYH1o/vz5Tee5xzHLoRQ9zmiZPn06ABdccEHTeVn1y9UvM9L5Vh7ucVKNEqPRvAsuuGDdFz7bnk26PPOtXJw4Kf/1t5Fw4rQgm1TTp08fcret2XyrFieOWQ4eHGjRcAf69cu416k29zgtaCUJnCj9pRSX3IzGCdCRDiW3soyHo8tt/vz5TS+5ceKYNTFU4nhXzSwHJ45ZDh5V6yFjZ4/doO2Z054pIBIbjnucHlFLmmdOe2bdI9tuvcWJY5ZD7sSRtEt606Rlku6RdHzafqqkxyUtSR+VuzeKWTvHOK8AJ0bEHZK2AG6XdF067+yI+Fr74Zn1ptyJExGrSO7ORUQ8L2kZrd9Wz6zUOnKMI2l34G3ALWnTDElLJV0kqeHRrSt5ri87GFB7ZNutt7Q9HC1pc169O/IaSecBXySp3PlFkjscf6r+fa7kuSEnSXm01eNIeg1J0lweEd8HiIjVEbE2Iv4IXEhSgN2sUtoZVRPwLWBZRJyVad8hs9gRJPeqNKuUdnbV9gc+BtwlaUnadjJwZHr35QAeAXy9vVVOO6Nq/0PjuxIsyh+O9SL/HGJDfXut2l33Hbne6zfvvXBE8zvxGa2so2jTp09vWHOh35PHl9zYkPo9QZpx4ljLhirY2G+cONYyF1J8lRPHhuQkacw1B2xY/TqqNlTNgb4dVbPW9UuijIR31cxycOKY5eDEMcuhb45x6u9x0+iMeKP52ees+rbaZ82e/UC3NqEjTjttr6JDqIS+6nGGO8ht5SA4e2OoVt9j1dNXiTPcOYn6+Y2Wb2UZq76+Spz63qLR/Prp+uUbvd+9Tv/pq8Spl+fuafXvaXT8Y9XnKwfMmujqlQOSHgGeB9YCr0TEvpK2Br4L7E7yK9APR4QrUVhldGpX7aCImJjJzlnA9RGxF3B9+tqsMrp1Hucw4N3p9CXAT4GTurSuERnJ+ZpG7Y3ek3XIL34xOhuS07UHHFB0CJXQicQJ4CfpccoFab207dNKn0TEKknjOrCejmn3FoRmndhV2z8iJgGHAMdJOrCVNxVZyXOk53PyLmPV1XbiRMTK9HkQuJqkAOHqWn219HmwwfsWRMS+zUYtummkVxA0e+3zN/2r3Uqem6V3KkDSZsD7SQoQXgMcnS52NPDDdtbTaY3OxQw136xeW+dxJO1B0stAcrz0nYj4sqRtgCuBXYFHgQ9FxNNDfI7P41jP6dp5nIh4CHhrg/angPe089lmvawUVw6YFaTcNQcmfWlS0SFYH7rjs3c0nVeKxBm3c0+dBjIrR+JsdGVfX8RtPagUibNk5yXDL2Q2ikqROON3HV90CNaHVrKy6TzvA5nlUIoex4MD1mt8HsesuabncbyrZpaDE8csh1Ic4yye5CsHbPRNuaP5lQPuccxycOKY5eDEMcuhFMc4Exf5ygErwBBfO/c4Zjnk7nEk7U1SrbNmD2AOsBXw98CTafvJEbEod4TARz8xZ4O22Sf+47rp0878t3Y+vi21OBxDFWNo/rXNnTgRcR8wEUDSGOBxkvoDnwTOjoiv5f3sVqw9ae2rLwq8ImddHI6hr2Lo1DHOe4AHI+K3kjr0kUMbc/qYV1+cOSqrHDoOx9BXMXQqcaYBCzOvZ0j6OHAbcGI3Cq67x3EMRcbQ9uCApE2AvwT+M206D9iTZDduFU3yv91KnmNOH7PuUSTH0J8xdKLHOQS4IyJWA9SeASRdCPyo0ZvSGtML0uVGfHW0exzHUGQMnUicI8nspknaoVZwHTiCpLJnx/kYxzEUGUNbiSPpT4D3Adl6sWdImkhyF4NH6uZ1jHscx1BkDO1W8nwB2Kau7WNtRdQi9ziOocgYSnHJTSPucRxDkTGUNnHc4ziGImMobeK4x3EMRcZQ2sRxj+MYioyhtInjHscxFBlDaRPHPY5jKDKG0iaOexzHUGQMpShI+MQTU0crFLN1xo9f5IKEZp1Uil21Gyf5Nh/WW9zjmOXgxDHLwYljlkMpjnEOumNi0SFYPxrvO7KZdVQpepxGddXMuq95XTX3OGY5tJQ4ki6SNCjp7kzb1pKuk/RA+jw2bZekr0taLmmpJN/cxiqn1R7nYmBKXdss4PqI2Au4Pn0NSdWbvdLHMSTloswqpaXEiYifA0/XNR8GXJJOXwIcnmm/NBI3A1tJ2qETwZr1inaOcbavlYFKn2vXpO4EPJZZbkXatp52CxKaFakbo2qNikdvcPVzuwUJzYrUTo+zurYLlj4Ppu0rgF0yy+0MND+TZFZC7STONcDR6fTRwA8z7R9PR9cmA89lKnuaVUJLu2qSFgLvBraVtAL4PDAPuFLSp4FHgQ+liy8CpgLLgRdI7pdjViktJU5EHNlk1nsaLBvAce0EZdbrfOWAWQ5OHLMcnDhmOThxzHJw4pjl4MQxy8GJY5aDE8csByeOWQ5OHLMcnDhmOThxzHJw4pjl4MQxy8GJY5aDE8csByeOWQ7DJk6TKp5flfSbtFLn1ZK2Stt3l/SipCXp4/xuBm9WlFZ6nIvZsIrndcCbIuItwP3A7My8ByNiYvo4tjNhmvWWYROnURXPiPhJRLySvryZpASUWd/oxDHOp4BrM68nSPq1pJ9JOqDZm1zJ08qsrUqekk4BXgEuT5tWAbtGxFOS3g78QNI+EbGm/r29VMnzhsWT100fPOXmAiPpjqpvXxFy9ziSjgYOBf42LQlFRLwcEU+l07cDDwJv6ESg3ZL9UvWDftvebsnV40iaApwEvCsiXsi0bwc8HRFrJe1BcquPhzoSaZfce++9G7TNmLlBB1laTpTuaGU4eiHwS2BvSSvSyp3nAlsA19UNOx8ILJV0J/A94NiIqL89SE+ofaEGBgYYGBhgxsw1zJi5hoGBgYIj67yDp9zsXbQOG7bHaVLF81tNlr0KuKrdoIpQS6SqfcEOnnKze50uKMXNczvtnKPmcgJJghw85WbOOWruunknfLuoqLpn6benrps+4du+EXEn9O0lN9lkMRupvk2crNpf4Sr9NT7nqLnr/jhUcfuKpnQkudgghjmP043jjjmzXgJg7rxNmTPrJebO27Tj6yjSuWe9Hlh/hLBRmzV3w+LJt0fEvo3m9XWPU0saeDWRqqRR0tRPWz59nTj1qpY8tQRxonRe3yZOtrepMidNd/Rt4mRV7fimno9pOs+JU1G1ZJkxcw3nnvV6J0+HOXEqrJY04F22Tuv7xOnH3TT3Pu3r+8TJDhBUNYmyiZLthSy/vrxWrWbcuHHp1Jp0uvp/iZ00ndHXiTMwMLDuqoTsdNXcsHgyA4vXb6vqto6Wvt9VM8ujrxMn++vPRr8ENWumr3fVaqq833/uWa+Hs6r98/Ai5K3keaqkxzMVO6dm5s2WtFzSfZI+0K3Au6FqCVS17ekleSt5Apydqdi5CEDSADAN2Cd9zzckjelUsJ02Y+Ya5sx6iRkz1zA4OMjg4GDRIVlJtFJz4OeSdm/x8w4DroiIl4GHJS0H9iMp9tGzqvh7nKF4N6197QwOzEiLrl8kaWzathPwWGaZFWnbBnqlkmctYap2pXR2N82J0nl5E+c8YE9gIkn1zjPTdjVYtuGvOyNiQUTs2+wXdkWoWvJY9+RKnIhYHRFrI+KPwIUku2OQ9DC7ZBbdGVjZXojWjvrexr1PZ+RKHEk7ZF4eAdRG3K4Bpkl6raQJJJU8f9VeiN3Vb72MR9o6I28lzzMk3SVpKXAQ8M8AEXEPcCVwL7AYOC4i1nYtehvWUEU7LL+OVvJMl/8y8OV2ghot/dbbWOf09SU39fplSNrHOe3r60tu5s7btLI1o2fMXLPBttUKyldtW4tQioKEPzp5YtfWfehXlozKeopy6FeWbLBdjdpsQ4d+ZYkLEjZT5S9Q9o/CUG02cqXoccwK0rTHKcUxTpV7BetdQ/XOfb+rZpaHE8csByeOWQ4eHDBrzoMDZiPlwQGzDivFrtoTT0wdarZZV4wfv6jcu2o3TvLZbust3lUzy8GJY5aDE8csh7yVPL+bqeL5iKQlafvukl7MzDu/m8GbFaWVwYGLgXOBS2sNEfGR2rSkM4HnMss/GBEdPfFy0B0+j2MFGN+8QFNblTwlCfgwcHDO0Foyfvyibn682Yi1Oxx9ALA6Ih7ItE2Q9GuS25t9NiJ+0eiNko4BjmllJQt33LHNMM1G7siVbfQ4w302sDDzehWwa0Q8JentwA8k7RMRG1SHiIgFwALwtWpWPrkTR9LGwF8Bb6+1pcXWX06nb5f0IPAGoGv1obPHP7UTpY3auskx9HYM3YijneHo9wK/iYgVtQZJ29Vu6yFpD5JKng+1F+LwGv1DjPbVBo6ht2PodBx5K3lCch+chXWLHwgslXQn8D3g2Ih4umPRmvWIvJU8iYhPNGi7Criq/bDMepuvHDDLoTKJk91/LepqasfQmzF0I45S/KxgOL1wZYFj6K8YSvFDNp8AtSIcuXJl0x+ylSJxzApS7l+AJteYDu+yP/0CAB/75ee7GYxjKFkM+eOY0XROZQYHzEaTE8csByeOWQ6lOMYZv+M2XV2+GxxD78QA+eJ4ovmvCtzjmOVRih5nu/Fjh13mrNM/x8yTLgPgsks+x8yTvtjtsBxDSWLIG0fle5zLL57H9ttvtu719ttvxuUXz3MMjqFrcZSjxxm3VUvL1f/DtPq+TnIMvRtDJ+MoxZUDrdxe/DsXz13v9Uc/Mae9oHJwDL0bQ544blg8udyX3LSSOGadNlTiVOIYx2y0tfLT6V0k3ShpmaR7JB2ftm8t6TpJD6TPY9N2Sfq6pOWSlkqa1O2NMBttrfQ4rwAnRsQbgcnAcZIGgFnA9RGxF3B9+hrgEJIiHXuR1E07r+NRmxVs2MSJiFURcUc6/TywDNgJOAy4JF3sEuDwdPow4NJI3AxsJWmHjkduVqARDUenpXDfBtwCbB8RqyBJLknj0sV2Ah7LvG1F2raq7rNaruR5w+LJIwnTrOtaThxJm5NUsDkhItYkZaMbL9qgbYNRM1fytDJraVRN0mtIkubyiPh+2ry6tguWPg+m7SuAXTJv3xkY4uIFs/JpZVRNwLeAZRFxVmbWNcDR6fTRwA8z7R9PR9cmA8/VdunMKiMihnwAf06yq7UUWJI+pgLbkIymPZA+b50uL+DfgQeBu4B9W1hH+OFHDz5ua/adLcWVA2YF8ZUDZp3kxDHLwYljloMTxyyHXvkh2++AP6TPVbEt1dmeKm0LtL49uzWb0ROjagCSbms2glFGVdqeKm0LdGZ7vKtmloMTxyyHXkqcBUUH0GFV2p4qbQt0YHt65hjHrEx6qccxKw0njlkOhSeOpCmS7kuLe8wa/h29R9Ijku6StETSbWlbw2ImvUjSRZIGJd2daSttMZYm23OqpMfT/6MlkqZm5s1Ot+c+SR9oaSXDXfLfzQcwhuTnB3sAmwB3AgNFxpRzOx4Btq1rOwOYlU7PAk4vOs4h4j8QmATcPVz8JD8puZbk5yOTgVuKjr/F7TkV+JcGyw6k37vXAhPS7+OY4dZRdI+zH7A8Ih6KiP8FriAp9lEFzYqZ9JyI+DnwdF1zaYuxNNmeZg4DroiIlyPiYWA5yfdySEUnTrPCHmUTwE8k3Z4WIYG6YibAuKbv7k3N4i/z/9mMdPfyosyuc67tKTpxWirsUQL7R8Qkkppyx0k6sOiAuqis/2fnAXsCE0kqLp2ZtufanqITpxKFPSJiZfo8CFxN0tU3K2ZSFpUqxhIRqyNibUT8EbiQV3fHcm1P0YlzK7CXpAmSNgGmkRT7KA1Jm0naojYNvB+4m+bFTMqiUsVY6o7DjiD5P4Jke6ZJeq2kCSQVaH817Af2wAjIVOB+ktGMU4qOJ0f8e5CMytwJ3FPbBpoUM+nFB7CQZPfl/0j+An+6WfzkKMbSI9tzWRrv0jRZdsgsf0q6PfcBh7SyDl9yY5ZD0btqZqXkxDHLwYljloMTxywHJ45ZDk4csxycOGY5/D/iiffy4bYBOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "for _ in range(100):\n",
    "    s, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.title('Game image')\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.show()\n",
    "\n",
    "plt.title('Agent observation')\n",
    "plt.imshow(s.reshape([42, 42]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POMDP setting\n",
    "\n",
    "The atari game we're working with is actually a POMDP: your agent needs to know timing at which enemies spawn and move, but cannot do so unless it has some memory. \n",
    "\n",
    "Let's design another agent that has a recurrent neural net memory to solve this. Here's a sketch.\n",
    "\n",
    "![img](img1.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# a special module that converts [batch, channel, w, h] to [batch, units]\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:5') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRecurrentAgent(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions, reuse=False):\n",
    "        \"\"\"A simple actor-critic agent\"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "\n",
    "        self.conv0 = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.conv1 = nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.activation = nn.ReLU()\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        self.hid = nn.Linear(512, 128)\n",
    "        self.rnn = nn.LSTMCell(128, 128)\n",
    "\n",
    "        self.logits = nn.Linear(128, n_actions)\n",
    "        self.state_value = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, prev_state, obs_t):\n",
    "        \"\"\"\n",
    "        Takes agent's previous step and observation, \n",
    "        returns next state and whatever it needs to learn (tf tensors)\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE: apply the whole neural net for one step here.\n",
    "        # See docs on self.rnn(...)\n",
    "        # the recurrent cell should take the last feedforward dense layer as input\n",
    "        \n",
    "        \n",
    "        emb = self.activation(self.conv0(obs_t))\n",
    "        emb = self.activation(self.conv1(emb))\n",
    "        emb = self.activation(self.conv2(emb))\n",
    "        \n",
    "        emb = self.hid(self.flatten(emb))\n",
    "        \n",
    "        new_state = self.rnn(emb, prev_state)\n",
    "        logits = self.logits(new_state[0])\n",
    "        state_value = self.state_value(new_state[0])\n",
    "\n",
    "        return new_state, (logits, state_value)\n",
    "\n",
    "    def get_initial_state(self, batch_size):\n",
    "        \"\"\"Return a list of agent memory states at game start. Each state is a np array of shape [batch_size, ...]\"\"\"\n",
    "        return torch.zeros((batch_size, 128)), torch.zeros((batch_size, 128))\n",
    "\n",
    "    def sample_actions(self, agent_outputs):\n",
    "        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"\n",
    "        logits, state_values = agent_outputs\n",
    "        probs = F.softmax(logits)\n",
    "        return torch.multinomial(probs, 1)[:, 0].data.numpy()\n",
    "\n",
    "    def step(self, prev_state, obs_t):\n",
    "        \"\"\" like forward, but obs_t is a numpy array \"\"\"\n",
    "        obs_t = torch.tensor(np.asarray(obs_t), dtype=torch.float32)\n",
    "        (h, c), (l, s) = self.forward(prev_state, obs_t)\n",
    "        return (h.detach(), c.detach()), (l.detach(), s.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parallel_games = 5\n",
    "gamma = 0.99\n",
    "\n",
    "agent = SimpleRecurrentAgent(obs_shape, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action logits:\n",
      " tensor([[-0.0354, -0.0695, -0.0433,  0.0111, -0.0581, -0.0731,  0.0421, -0.0686,\n",
      "          0.0931,  0.0780, -0.0888,  0.0676, -0.0459,  0.0558]])\n",
      "state values:\n",
      " tensor([[-0.0071]])\n"
     ]
    }
   ],
   "source": [
    "state = [env.reset()]\n",
    "_, (logits, value) = agent.step(agent.get_initial_state(1), state)\n",
    "print(\"action logits:\\n\", logits)\n",
    "print(\"state values:\\n\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play!\n",
    "Let's build a function that measures agent's average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_games=1):\n",
    "    \"\"\"Plays an entire game start to end, returns session rewards.\"\"\"\n",
    "\n",
    "    game_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        # initial observation and memory\n",
    "        observation = env.reset()\n",
    "        prev_memories = agent.get_initial_state(1)\n",
    "\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            new_memories, readouts = agent.step(\n",
    "                prev_memories, observation[None, ...])\n",
    "            action = agent.sample_actions(readouts)\n",
    "\n",
    "            observation, reward, done, info = env.step(action[0])\n",
    "\n",
    "            total_reward += reward\n",
    "            prev_memories = new_memories\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        game_rewards.append(total_reward)\n",
    "    return game_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3fe6c840f544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv_monitor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"kungfu_videos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_monitor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_games\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0menv_monitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-dbd96891fb54>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(agent, env, n_games)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_games\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# initial observation and memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mprev_memories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_after_reset\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         )\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_ansi_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_image_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m_encode_image_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode_image_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes_per_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoder_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_path, frame_shape, frames_per_sec)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ffmpeg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m: Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`."
     ]
    }
   ],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "rw = evaluate(agent, env_monitor, n_games=3,)\n",
    "env_monitor.close()\n",
    "print(rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./kungfu_videos/openaigym.video.0.1054.video000000.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s: s.endswith(\n",
    "    \".mp4\"), os.listdir(\"./kungfu_videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on parallel games\n",
    "\n",
    "We introduce a class called EnvPool - it's a tool that handles multiple environments for you. Here's how it works:\n",
    "![img](img2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_pool import EnvPool\n",
    "pool = EnvPool(agent, make_env, n_parallel_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gonna train our agent on a thing called __rollouts:__\n",
    "![img](img3.jpg)\n",
    "\n",
    "A rollout is just a sequence of T observations, actions and rewards that agent took consequently.\n",
    "* First __s0__ is not necessarily initial state for the environment\n",
    "* Final state is not necessarily terminal\n",
    "* We sample several parallel rollouts for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romakail/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# for each of n_parallel_games, take 10 steps\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions shape: (5, 10)\n",
      "Rewards shape: (5, 10)\n",
      "Mask shape: (5, 10)\n",
      "Observations shape:  (5, 10, 1, 42, 42)\n"
     ]
    }
   ],
   "source": [
    "print(\"Actions shape:\", rollout_actions.shape)\n",
    "print(\"Rewards shape:\", rollout_rewards.shape)\n",
    "print(\"Mask shape:\", rollout_mask.shape)\n",
    "print(\"Observations shape: \", rollout_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic objective\n",
    "\n",
    "Here we define a loss function that uses rollout above to train advantage actor-critic agent.\n",
    "\n",
    "\n",
    "Our loss consists of three components:\n",
    "\n",
    "* __The policy \"loss\"__\n",
    " $$ \\hat J = {1 \\over T} \\cdot \\sum_t { \\log \\pi(a_t | s_t) } \\cdot A_{const}(s,a) $$\n",
    "  * This function has no meaning in and of itself, but it was built such that\n",
    "  * $ \\nabla \\hat J = {1 \\over N} \\cdot \\sum_t { \\nabla \\log \\pi(a_t | s_t) } \\cdot A(s,a) \\approx \\nabla E_{s, a \\sim \\pi} R(s,a) $\n",
    "  * Therefore if we __maximize__ J_hat with gradient descent we will maximize expected reward\n",
    "  \n",
    "  \n",
    "* __The value \"loss\"__\n",
    "  $$ L_{td} = {1 \\over T} \\cdot \\sum_t { [r + \\gamma \\cdot V_{const}(s_{t+1}) - V(s_t)] ^ 2 }$$\n",
    "  * Ye Olde TD_loss from q-learning and alike\n",
    "  * If we minimize this loss, V(s) will converge to $V_\\pi(s) = E_{a \\sim \\pi(a | s)} R(s,a) $\n",
    "\n",
    "\n",
    "* __Entropy Regularizer__\n",
    "  $$ H = - {1 \\over T} \\sum_t \\sum_a {\\pi(a|s_t) \\cdot \\log \\pi (a|s_t)}$$\n",
    "  * If we __maximize__ entropy we discourage agent from predicting zero probability to actions\n",
    "  prematurely (a.k.a. exploration)\n",
    "  \n",
    "  \n",
    "So we optimize a linear combination of $L_{td}$ $- \\hat J$, $-H$\n",
    "  \n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "__One more thing:__ since we train on T-step rollouts, we can use N-step formula for advantage for free:\n",
    "  * At the last step, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot V(s_{t+1}) - V(s) $\n",
    "  * One step earlier, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot r(s_{t+1}, a_{t+1}) + \\gamma ^ 2 \\cdot V(s_{t+2}) - V(s) $\n",
    "  * Et cetera, et cetera. This way agent starts training much faster since it's estimate of A(s,a) depends less on his (imperfect) value function and more on actual rewards. There's also a [nice generalization](https://arxiv.org/abs/1506.02438) of this.\n",
    "\n",
    "\n",
    "__Note:__ it's also a good idea to scale rollout_len up to learn longer sequences. You may wish set it to >=20 or to start at 10 and then scale up as time passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take an integer tensor and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y.to(dtype=torch.int64).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "def train_on_rollout(states, actions, rewards, is_not_done, prev_memory_states, gamma=0.99, device=device, max_grad_norm=90):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # shape: [batch_size, time, c, h, w]\n",
    "    states = torch.tensor(np.asarray(states), dtype=torch.float32)\n",
    "    actions = torch.tensor(np.array(actions), dtype=torch.int64)  # shape: [batch_size, time]\n",
    "    rewards = torch.tensor(np.array(rewards), dtype=torch.float32)  # shape: [batch_size, time]\n",
    "    is_not_done = torch.tensor(np.array(is_not_done), dtype=torch.float32)  # shape: [batch_size, time]\n",
    "    rollout_length = rewards.shape[1] - 1\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    memory = [m.detach() for m in prev_memory_states]\n",
    "\n",
    "    logits = []  # append logit sequence here\n",
    "    state_values = []  # append state values here\n",
    "    for t in range(rewards.shape[1]):\n",
    "        obs_t = states[:, t]\n",
    "\n",
    "        # use agent to comute logits_t and state values_t.\n",
    "        # append them to logits and state_values array\n",
    "\n",
    "        memory, (logits_t, values_t) = agent(memory, obs_t)#<YOUR CODE >\n",
    "\n",
    "        logits.append(logits_t)\n",
    "        state_values.append(values_t)\n",
    "\n",
    "    logits = torch.stack(logits, dim=1)\n",
    "    state_values = torch.stack(state_values, dim=1)\n",
    "    probas = F.softmax(logits, dim=2)\n",
    "    logprobas = F.log_softmax(logits, dim=2)\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    actions_one_hot = to_one_hot(actions, n_actions).view(\n",
    "        actions.shape[0], actions.shape[1], n_actions)\n",
    "    logprobas_for_actions = torch.sum(logprobas * actions_one_hot, dim=-1)\n",
    "\n",
    "    # Now let's compute two loss components:\n",
    "    # 1) Policy gradient objective.\n",
    "    # Notes: Please don't forget to call .detach() on advantage term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    J_hat = 0  # policy objective as in the formula for J_hat\n",
    "\n",
    "    # 2) Temporal difference MSE for state values\n",
    "    # Notes: Please don't forget to call on V(s') term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    value_loss = 0\n",
    "\n",
    "    cumulative_returns = state_values[:, -1].detach()\n",
    "\n",
    "    for t in reversed(range(rollout_length)):\n",
    "        r_t = rewards[:, t].unsqueeze(1)                               # current rewards\n",
    "        # current state values\n",
    "        V_t = state_values[:, t]\n",
    "        V_next = state_values[:, t + 1].detach()           # next state values\n",
    "        # log-probability of a_t in s_t\n",
    "        logpi_a_s_t = logprobas_for_actions[:, t].unsqueeze(1)\n",
    "\n",
    "        # update G_t = r_t + gamma * G_{t+1} as we did in week6 reinforce\n",
    "        cumulative_returns = G_t = r_t + gamma * cumulative_returns\n",
    "\n",
    "        # Compute temporal difference error (MSE for V(s))\n",
    "#         print ('Value loss :')\n",
    "#         print ('r_t :', r_t.shape)\n",
    "#         print ('V_next :', V_next.shape)\n",
    "#         print ('V_t :', V_t.shape)\n",
    "        \n",
    "        value_loss += (r_t + gamma * V_next - V_t) ** 2#<YOUR CODE >\n",
    "#         print ('Value loss :', value_loss.shape)\n",
    "        \n",
    "        # compute advantage A(s_t, a_t) using cumulative returns and V(s_t) as baseline\n",
    "        advantage = G_t - V_t #<YOUR CODE >\n",
    "        advantage = advantage.detach()\n",
    "\n",
    "        # compute policy pseudo-loss aka -J_hat.\n",
    "#         print ('logpi_a_s_t', logpi_a_s_t.shape)\n",
    "#         print ('advantage', advantage.shape)\n",
    "        J_hat += logpi_a_s_t * advantage  #<YOUR CODE >\n",
    "#         print ('J_hat :', J_hat.shape)\n",
    "        \n",
    "    # regularize with entropy\n",
    "#     print ('logprobas', (logprobas * probas).shape)\n",
    "    entropy_reg = - (logprobas * probas).sum(dim=2).mean(dim=1).unsqueeze(1)#<compute entropy regularizer >\n",
    "\n",
    "    # add-up three loss components and average over time\n",
    "#     print ('J_hat :', J_hat.shape)\n",
    "#     print ('value_loss :', value_loss.shape)\n",
    "#     print ('entropy_reg :', entropy_reg.shape)\n",
    "    \n",
    "    loss = -J_hat / rollout_length +\\\n",
    "        value_loss / rollout_length +\\\n",
    "           -0.01 * entropy_reg\n",
    "    loss = loss.mean()\n",
    "\n",
    "    # Gradient descent step\n",
    "    loss.backward()\n",
    "    # This small trick allows to clip gradients and to monitor them over the time\n",
    "    grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "    #< your code >\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    \n",
    "    return loss.data.numpy(), grad_norm, entropy_reg.mean().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romakail/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(-0.02418448, dtype=float32),\n",
       " 0.0012854373709777485,\n",
       " array(2.6367955, dtype=float32))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's test it\n",
    "memory = list(pool.prev_memory_states)\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)\n",
    "\n",
    "train_on_rollout(rollout_obs, rollout_actions,\n",
    "                 rollout_rewards, rollout_mask, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train \n",
    "\n",
    "just run train step and see if agent learns any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import trange\n",
    "from pandas import DataFrame\n",
    "moving_average = lambda x, **kw: DataFrame(\n",
    "    {'x': np.asarray(x)}).x.ewm(**kw).mean().values\n",
    "\n",
    "rewards_history = []\n",
    "grad_norm_history = []\n",
    "entropy_history = []\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romakail/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "for i in range(15000, 25000):\n",
    "\n",
    "    memory = list(pool.prev_memory_states)\n",
    "    rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(\n",
    "        20)\n",
    "    loss, grad_norm, entropy = train_on_rollout(rollout_obs, rollout_actions,\n",
    "                     rollout_rewards, rollout_mask, memory)\n",
    "    grad_norm_history.append(grad_norm)\n",
    "    entropy_history.append(entropy)\n",
    "    loss_history.append(loss)\n",
    "    if i % 100 == 0:\n",
    "        rewards_history.append(np.mean(evaluate(agent, env, n_games=1)))\n",
    "        clear_output(True)\n",
    "        \n",
    "        plt.figure(figsize=[16, 9])\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.title(\"Mean reward\")\n",
    "        plt.plot(rewards_history, label='rewards')\n",
    "        plt.plot(moving_average(np.array(rewards_history),\n",
    "                                span=10), label='rewards ewma@10')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.title(\"Grad norm history (smoothened)\")\n",
    "        plt.plot(moving_average(np.array(grad_norm_history), span=100), label='grad norm ewma@100')\n",
    "        plt.grid()\n",
    "        \n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.title(\"entropy (smoothened)\")\n",
    "        plt.plot(moving_average(np.array(entropy_history), span=100), label='entropy ewma@100')\n",
    "        plt.grid()\n",
    "        \n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.title(\"loss (smoothened)\")\n",
    "        plt.plot(np.array(loss_history), label='loss raw')\n",
    "        plt.plot(moving_average(np.array(loss_history), span=10), label='loss ewma@10')\n",
    "        plt.grid()\n",
    "\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        if rewards_history[-1] >= 10000:\n",
    "            print(\"Your agent has just passed the minimum homework threshold\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relax and grab some refreshments while your agent is locked in an infinite loop of violence and death.\n",
    "\n",
    "__How to interpret plots:__\n",
    "\n",
    "The session reward is the easy thing: it should in general go up over time, but it's okay if it fluctuates ~~like crazy~~. It's also OK if it reward doesn't increase substantially before some 10k initial steps. However, if reward reaches zero and doesn't seem to get up over 2-3 evaluations, there's something wrong happening.\n",
    "\n",
    "\n",
    "Since we use a policy-based method, we also keep track of __policy entropy__ - the same one you used as a regularizer. The only important thing about it is that your entropy shouldn't drop too low (`< 0.1`) before your agent gets the yellow belt. Or at least it can drop there, but _it shouldn't stay there for long_.\n",
    "\n",
    "If it does, the culprit is likely:\n",
    "* Some bug in entropy computation. Remember that it is $ - \\sum p(a_i) \\cdot log p(a_i) $\n",
    "* Your agent architecture converges too fast. Increase entropy coefficient in actor loss. \n",
    "* Gradient explosion - just [clip gradients](https://stackoverflow.com/a/43486487) and maybe use a smaller network\n",
    "* Us. Or TF developers. Or aliens. Or lizardfolk. Contact us on forums before it's too late!\n",
    "\n",
    "If you're debugging, just run `logits, values = agent.step(batch_states)` and manually look into logits and values. This will reveal the problem 9 times out of 10: you'll likely see some NaNs or insanely large numbers or zeros. Try to catch the moment when this happens for the first time and investigate from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Final\" evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "final_rewards = evaluate(agent, env_monitor, n_games=20,)\n",
    "env_monitor.close()\n",
    "print(\"Final mean reward\", np.mean(final_rewards))\n",
    "\n",
    "video_names = list(filter(lambda s: s.endswith(\n",
    "    \".mp4\"), os.listdir(\"./kungfu_videos/\")))\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
