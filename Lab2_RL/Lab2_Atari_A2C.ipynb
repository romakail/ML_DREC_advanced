{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Mastering A2C (and Kung-Fu)\n",
    "\n",
    "This part is based on [Practical RL week08 practice](https://github.com/yandexdataschool/Practical_RL/tree/master/week08_pomdp). All rights belong to original authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Please, note that supplementary files require tensorflow version 1.15.0 (while TF 2.0 is actually released now). Please downgrade your TF or create new environment.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This versions shpuld work fine\n",
    "\n",
    "# ! pip install scipy==1.0.1\n",
    "# ! pip install tensorflow==1.15.0 --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from IPython.core import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "# if you're running in colab\n",
    "# !wget https://raw.githubusercontent.com/yandexdataschool/Practical_RL/0ccb0673965dd650d9b284e1ec90c2bfd82c8a94/week08_pomdp/atari_util.py\n",
    "# !wget https://raw.githubusercontent.com/yandexdataschool/Practical_RL/0ccb0673965dd650d9b284e1ec90c2bfd82c8a94/week08_pomdp/env_pool.py\n",
    "\n",
    "# If you are running on a server, launch xvfb to record game videos\n",
    "# Please make sure you have xvfb installed\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    print ('We are displaying')\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kung-Fu, recurrent style\n",
    "\n",
    "In this notebook we'll once again train RL agent for for atari [KungFuMaster](https://gym.openai.com/envs/KungFuMaster-v0/), this time using recurrent neural networks.\n",
    "\n",
    "![http://www.retroland.com/wp-content/uploads/2011/07/King-Fu-Master.jpg](http://www.retroland.com/wp-content/uploads/2011/07/King-Fu-Master.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Observation shape: (1, 42, 42)\n",
      "Num actions: 14\n",
      "Action names: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roma/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from atari_util import PreprocessAtari\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"KungFuMasterDeterministic-v0\")\n",
    "    env = PreprocessAtari(env, height=42, width=42,\n",
    "                          crop=lambda img: img[60:-30, 15:],\n",
    "                          color=False, n_frames=1)\n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation shape:\", obs_shape)\n",
    "print(\"Num actions:\", n_actions)\n",
    "print(\"Action names:\", env.env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "for _ in range(100):\n",
    "    s, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.title('Game image')\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.show()\n",
    "\n",
    "plt.title('Agent observation')\n",
    "plt.imshow(s.reshape([42, 42]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POMDP setting\n",
    "\n",
    "The atari game we're working with is actually a POMDP: your agent needs to know timing at which enemies spawn and move, but cannot do so unless it has some memory. \n",
    "\n",
    "Let's design another agent that has a recurrent neural net memory to solve this. Here's a sketch.\n",
    "\n",
    "![img](img1.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# a special module that converts [batch, channel, w, h] to [batch, units]\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRecurrentAgent(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions, reuse=False):\n",
    "        \"\"\"A simple actor-critic agent\"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "\n",
    "        self.conv0 = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.conv1 = nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.activation = nn.ReLU()\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        self.hid = nn.Linear(512, 128)\n",
    "        self.rnn = nn.LSTMCell(128, 128)\n",
    "\n",
    "        self.logits = nn.Linear(128, n_actions)\n",
    "        self.state_value = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, prev_state, obs_t):\n",
    "        \"\"\"\n",
    "        Takes agent's previous step and observation, \n",
    "        returns next state and whatever it needs to learn (tf tensors)\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE: apply the whole neural net for one step here.\n",
    "        # See docs on self.rnn(...)\n",
    "        # the recurrent cell should take the last feedforward dense layer as input\n",
    "        \n",
    "        \n",
    "        emb = self.activation(self.conv0(obs_t))\n",
    "        emb = self.activation(self.conv1(emb))\n",
    "        emb = self.activation(self.conv2(emb))\n",
    "        \n",
    "        emb = self.hid(self.flatten(emb))\n",
    "        \n",
    "        new_state = self.rnn(emb, prev_state)\n",
    "        logits = self.logits(new_state[0])\n",
    "        state_value = self.state_value(new_state[0])\n",
    "\n",
    "        return new_state, (logits, state_value)\n",
    "\n",
    "    def get_initial_state(self, batch_size):\n",
    "        \"\"\"Return a list of agent memory states at game start. Each state is a np array of shape [batch_size, ...]\"\"\"\n",
    "        return torch.zeros((batch_size, 128)), torch.zeros((batch_size, 128))\n",
    "\n",
    "    def sample_actions(self, agent_outputs):\n",
    "        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"\n",
    "        logits, state_values = agent_outputs\n",
    "        probs = F.softmax(logits)\n",
    "        return torch.multinomial(probs, 1)[:, 0].data.numpy()\n",
    "\n",
    "    def step(self, prev_state, obs_t):\n",
    "        \"\"\" like forward, but obs_t is a numpy array \"\"\"\n",
    "        obs_t = torch.tensor(np.asarray(obs_t), dtype=torch.float32)\n",
    "        (h, c), (l, s) = self.forward(prev_state, obs_t)\n",
    "        return (h.detach(), c.detach()), (l.detach(), s.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parallel_games = 5\n",
    "gamma = 0.99\n",
    "\n",
    "agent = SimpleRecurrentAgent(obs_shape, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action logits:\n",
      " tensor([[ 0.0331,  0.0229,  0.0707, -0.0450,  0.0436,  0.0767,  0.0667,  0.0294,\n",
      "          0.0275,  0.0820, -0.0368, -0.0078, -0.0717,  0.0391]])\n",
      "state values:\n",
      " tensor([[0.0216]])\n"
     ]
    }
   ],
   "source": [
    "state = [env.reset()]\n",
    "_, (logits, value) = agent.step(agent.get_initial_state(1), state)\n",
    "print(\"action logits:\\n\", logits)\n",
    "print(\"state values:\\n\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play!\n",
    "Let's build a function that measures agent's average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_games=1):\n",
    "    \"\"\"Plays an entire game start to end, returns session rewards.\"\"\"\n",
    "\n",
    "    game_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        # initial observation and memory\n",
    "        observation = env.reset()\n",
    "        prev_memories = agent.get_initial_state(1)\n",
    "\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            new_memories, readouts = agent.step(\n",
    "                prev_memories, observation[None, ...])\n",
    "            action = agent.sample_actions(readouts)\n",
    "\n",
    "            observation, reward, done, info = env.step(action[0])\n",
    "\n",
    "            total_reward += reward\n",
    "            prev_memories = new_memories\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        game_rewards.append(total_reward)\n",
    "    return game_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roma/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3fe6c840f544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv_monitor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"kungfu_videos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_monitor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_games\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0menv_monitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-dbd96891fb54>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(agent, env, n_games)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_after_step\u001b[0;34m(self, observation, reward, done, info)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# Record video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_ansi_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_image_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m_encode_image_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidFrame\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tried to pass invalid video frame, marking as broken: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLooseVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLooseVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1.9.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "rw = evaluate(agent, env_monitor, n_games=3,)\n",
    "env_monitor.close()\n",
    "print(rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s: s.endswith(\n",
    "    \".mp4\"), os.listdir(\"./kungfu_videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on parallel games\n",
    "\n",
    "We introduce a class called EnvPool - it's a tool that handles multiple environments for you. Here's how it works:\n",
    "![img](img2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_pool import EnvPool\n",
    "pool = EnvPool(agent, make_env, n_parallel_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gonna train our agent on a thing called __rollouts:__\n",
    "![img](img3.jpg)\n",
    "\n",
    "A rollout is just a sequence of T observations, actions and rewards that agent took consequently.\n",
    "* First __s0__ is not necessarily initial state for the environment\n",
    "* Final state is not necessarily terminal\n",
    "* We sample several parallel rollouts for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of n_parallel_games, take 10 steps\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actions shape:\", rollout_actions.shape)\n",
    "print(\"Rewards shape:\", rollout_rewards.shape)\n",
    "print(\"Mask shape:\", rollout_mask.shape)\n",
    "print(\"Observations shape: \", rollout_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic objective\n",
    "\n",
    "Here we define a loss function that uses rollout above to train advantage actor-critic agent.\n",
    "\n",
    "\n",
    "Our loss consists of three components:\n",
    "\n",
    "* __The policy \"loss\"__\n",
    " $$ \\hat J = {1 \\over T} \\cdot \\sum_t { \\log \\pi(a_t | s_t) } \\cdot A_{const}(s,a) $$\n",
    "  * This function has no meaning in and of itself, but it was built such that\n",
    "  * $ \\nabla \\hat J = {1 \\over N} \\cdot \\sum_t { \\nabla \\log \\pi(a_t | s_t) } \\cdot A(s,a) \\approx \\nabla E_{s, a \\sim \\pi} R(s,a) $\n",
    "  * Therefore if we __maximize__ J_hat with gradient descent we will maximize expected reward\n",
    "  \n",
    "  \n",
    "* __The value \"loss\"__\n",
    "  $$ L_{td} = {1 \\over T} \\cdot \\sum_t { [r + \\gamma \\cdot V_{const}(s_{t+1}) - V(s_t)] ^ 2 }$$\n",
    "  * Ye Olde TD_loss from q-learning and alike\n",
    "  * If we minimize this loss, V(s) will converge to $V_\\pi(s) = E_{a \\sim \\pi(a | s)} R(s,a) $\n",
    "\n",
    "\n",
    "* __Entropy Regularizer__\n",
    "  $$ H = - {1 \\over T} \\sum_t \\sum_a {\\pi(a|s_t) \\cdot \\log \\pi (a|s_t)}$$\n",
    "  * If we __maximize__ entropy we discourage agent from predicting zero probability to actions\n",
    "  prematurely (a.k.a. exploration)\n",
    "  \n",
    "  \n",
    "So we optimize a linear combination of $L_{td}$ $- \\hat J$, $-H$\n",
    "  \n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "__One more thing:__ since we train on T-step rollouts, we can use N-step formula for advantage for free:\n",
    "  * At the last step, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot V(s_{t+1}) - V(s) $\n",
    "  * One step earlier, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot r(s_{t+1}, a_{t+1}) + \\gamma ^ 2 \\cdot V(s_{t+2}) - V(s) $\n",
    "  * Et cetera, et cetera. This way agent starts training much faster since it's estimate of A(s,a) depends less on his (imperfect) value function and more on actual rewards. There's also a [nice generalization](https://arxiv.org/abs/1506.02438) of this.\n",
    "\n",
    "\n",
    "__Note:__ it's also a good idea to scale rollout_len up to learn longer sequences. You may wish set it to >=20 or to start at 10 and then scale up as time passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take an integer tensor and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y.to(dtype=torch.int64).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "def train_on_rollout(states, actions, rewards, is_not_done, prev_memory_states, gamma=0.99, device=device, max_grad_norm=90):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # shape: [batch_size, time, c, h, w]\n",
    "    states = torch.tensor(np.asarray(states), dtype=torch.float32)\n",
    "    actions = torch.tensor(np.array(actions), dtype=torch.int64)  # shape: [batch_size, time]\n",
    "    rewards = torch.tensor(np.array(rewards), dtype=torch.float32)  # shape: [batch_size, time]\n",
    "    is_not_done = torch.tensor(np.array(is_not_done), dtype=torch.float32)  # shape: [batch_size, time]\n",
    "    rollout_length = rewards.shape[1] - 1\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    memory = [m.detach() for m in prev_memory_states]\n",
    "\n",
    "    logits = []  # append logit sequence here\n",
    "    state_values = []  # append state values here\n",
    "    for t in range(rewards.shape[1]):\n",
    "        obs_t = states[:, t]\n",
    "\n",
    "        # use agent to comute logits_t and state values_t.\n",
    "        # append them to logits and state_values array\n",
    "\n",
    "        memory, (logits_t, values_t) = <YOUR CODE >\n",
    "\n",
    "        logits.append(logits_t)\n",
    "        state_values.append(values_t)\n",
    "\n",
    "    logits = torch.stack(logits, dim=1)\n",
    "    state_values = torch.stack(state_values, dim=1)\n",
    "    probas = F.softmax(logits, dim=2)\n",
    "    logprobas = F.log_softmax(logits, dim=2)\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    actions_one_hot = to_one_hot(actions, n_actions).view(\n",
    "        actions.shape[0], actions.shape[1], n_actions)\n",
    "    logprobas_for_actions = torch.sum(logprobas * actions_one_hot, dim=-1)\n",
    "\n",
    "    # Now let's compute two loss components:\n",
    "    # 1) Policy gradient objective.\n",
    "    # Notes: Please don't forget to call .detach() on advantage term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    J_hat = 0  # policy objective as in the formula for J_hat\n",
    "\n",
    "    # 2) Temporal difference MSE for state values\n",
    "    # Notes: Please don't forget to call on V(s') term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    value_loss = 0\n",
    "\n",
    "    cumulative_returns = state_values[:, -1].detach()\n",
    "\n",
    "    for t in reversed(range(rollout_length)):\n",
    "        r_t = rewards[:, t]                                # current rewards\n",
    "        # current state values\n",
    "        V_t = state_values[:, t]\n",
    "        V_next = state_values[:, t + 1].detach()           # next state values\n",
    "        # log-probability of a_t in s_t\n",
    "        logpi_a_s_t = logprobas_for_actions[:, t]\n",
    "\n",
    "        # update G_t = r_t + gamma * G_{t+1} as we did in week6 reinforce\n",
    "        cumulative_returns = G_t = r_t + gamma * cumulative_returns\n",
    "\n",
    "        # Compute temporal difference error (MSE for V(s))\n",
    "        value_loss += <YOUR CODE >\n",
    "\n",
    "        # compute advantage A(s_t, a_t) using cumulative returns and V(s_t) as baseline\n",
    "        advantage = <YOUR CODE >\n",
    "        advantage = advantage.detach()\n",
    "\n",
    "        # compute policy pseudo-loss aka -J_hat.\n",
    "        J_hat += <YOUR CODE >\n",
    "\n",
    "    # regularize with entropy\n",
    "    entropy_reg = <compute entropy regularizer >\n",
    "\n",
    "    # add-up three loss components and average over time\n",
    "    loss = -J_hat / rollout_length +\\\n",
    "        value_loss / rollout_length +\\\n",
    "           -0.01 * entropy_reg\n",
    "\n",
    "    # Gradient descent step\n",
    "    loss.backward()\n",
    "    # This small trick allows to clip gradients and to monitor them over the time\n",
    "    grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "    < your code >\n",
    "\n",
    "    return loss.data.numpy(), grad_norm, entropy_reg.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's test it\n",
    "memory = list(pool.prev_memory_states)\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)\n",
    "\n",
    "train_on_rollout(rollout_obs, rollout_actions,\n",
    "                 rollout_rewards, rollout_mask, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train \n",
    "\n",
    "just run train step and see if agent learns any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import trange\n",
    "from pandas import DataFrame\n",
    "moving_average = lambda x, **kw: DataFrame(\n",
    "    {'x': np.asarray(x)}).x.ewm(**kw).mean().values\n",
    "\n",
    "rewards_history = []\n",
    "grad_norm_history = []\n",
    "entropy_history = []\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15000, 25000):\n",
    "\n",
    "    memory = list(pool.prev_memory_states)\n",
    "    rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(\n",
    "        20)\n",
    "    loss, grad_norm, entropy = train_on_rollout(rollout_obs, rollout_actions,\n",
    "                     rollout_rewards, rollout_mask, memory)\n",
    "    grad_norm_history.append(grad_norm)\n",
    "    entropy_history.append(entropy)\n",
    "    loss_history.append(loss)\n",
    "    if i % 100 == 0:\n",
    "        rewards_history.append(np.mean(evaluate(agent, env, n_games=1)))\n",
    "        clear_output(True)\n",
    "        \n",
    "        plt.figure(figsize=[16, 9])\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.title(\"Mean reward\")\n",
    "        plt.plot(rewards_history, label='rewards')\n",
    "        plt.plot(moving_average(np.array(rewards_history),\n",
    "                                span=10), label='rewards ewma@10')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.title(\"Grad norm history (smoothened)\")\n",
    "        plt.plot(moving_average(np.array(grad_norm_history), span=100), label='grad norm ewma@100')\n",
    "        plt.grid()\n",
    "        \n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.title(\"entropy (smoothened)\")\n",
    "        plt.plot(moving_average(np.array(entropy_history), span=100), label='entropy ewma@100')\n",
    "        plt.grid()\n",
    "        \n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.title(\"loss (smoothened)\")\n",
    "        plt.plot(np.array(loss_history), label='loss raw')\n",
    "        plt.plot(moving_average(np.array(loss_history), span=10), label='loss ewma@10')\n",
    "        plt.grid()\n",
    "\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        if rewards_history[-1] >= 10000:\n",
    "            print(\"Your agent has just passed the minimum homework threshold\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relax and grab some refreshments while your agent is locked in an infinite loop of violence and death.\n",
    "\n",
    "__How to interpret plots:__\n",
    "\n",
    "The session reward is the easy thing: it should in general go up over time, but it's okay if it fluctuates ~~like crazy~~. It's also OK if it reward doesn't increase substantially before some 10k initial steps. However, if reward reaches zero and doesn't seem to get up over 2-3 evaluations, there's something wrong happening.\n",
    "\n",
    "\n",
    "Since we use a policy-based method, we also keep track of __policy entropy__ - the same one you used as a regularizer. The only important thing about it is that your entropy shouldn't drop too low (`< 0.1`) before your agent gets the yellow belt. Or at least it can drop there, but _it shouldn't stay there for long_.\n",
    "\n",
    "If it does, the culprit is likely:\n",
    "* Some bug in entropy computation. Remember that it is $ - \\sum p(a_i) \\cdot log p(a_i) $\n",
    "* Your agent architecture converges too fast. Increase entropy coefficient in actor loss. \n",
    "* Gradient explosion - just [clip gradients](https://stackoverflow.com/a/43486487) and maybe use a smaller network\n",
    "* Us. Or TF developers. Or aliens. Or lizardfolk. Contact us on forums before it's too late!\n",
    "\n",
    "If you're debugging, just run `logits, values = agent.step(batch_states)` and manually look into logits and values. This will reveal the problem 9 times out of 10: you'll likely see some NaNs or insanely large numbers or zeros. Try to catch the moment when this happens for the first time and investigate from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Final\" evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "final_rewards = evaluate(agent, env_monitor, n_games=20,)\n",
    "env_monitor.close()\n",
    "print(\"Final mean reward\", np.mean(final_rewards))\n",
    "\n",
    "video_names = list(filter(lambda s: s.endswith(\n",
    "    \".mp4\"), os.listdir(\"./kungfu_videos/\")))\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
